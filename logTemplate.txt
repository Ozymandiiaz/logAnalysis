/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/client/HdfsUtils.java	Is namenode in safemode? .*; uri=.*	safemode	uri	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/client/HdfsUtils.java	Got an exception for uri=.*	uri	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java	.*	String.format("addressKey: %s nsId: %s nnId: %s",addressKey,nsId,nnId)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java	Exception in creating socket address .*	addr	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java	Setting password to null since IOException is caught when getting password	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java	SSL config .* is missing. If .* is specified, make sure it is a relative path	sslProp	DFSConfigKeys.DFS_SERVER_HTTPS_KEYSTORE_RESOURCE_KEY	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java	Starting web server as: .*	SecurityUtil.getServerPrincipal(conf.get(spnegoUserNameKey),httpAddr.getHostName())	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java	Starting Web-server for .* at: .*	name	uri	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java	Starting Web-server for .* at: .*	name	uri	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HAUtil.java	Mapped HA service delegation token for logical URI .* to namenode .*	haUri	singleNNAddr	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/HAUtil.java	No HA service delegation token found for logical URI .*	haUri	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/NameNodeProxies.java	.*	message	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DomainPeerServer.java	error closing DomainPeerServer: 	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/TcpPeerServer.java	error closing TcpPeerServer: 	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferServer.java	SASL server doing encrypted handshake for peer = .*, datanodeId = .*	peer	datanodeId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferServer.java	SASL server skipping handshake in unsecured configuration for peer = .*, datanodeId = .*	peer	datanodeId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferServer.java	SASL server skipping handshake in secured configuration for peer = .*, datanodeId = .*	peer	datanodeId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferServer.java	SASL server doing general handshake for peer = .*, datanodeId = .*	peer	datanodeId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferServer.java	SASL server skipping handshake in secured configuration with no SASL protection configured for peer = .*, datanodeId = .*	peer	datanodeId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferServer.java	Server using encryption algorithm .*	dnConf.getEncryptionAlgorithm()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/protocol/datatransfer/sasl/SaslDataTransferServer.java	Server using cipher suite .*	cipherOption.getCipherSuite().getName()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java	Beginning recovery of unclosed segment starting at txid .*	segmentTxId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java	Recovery prepare phase complete. Responses:\n.*	QuorumCall.mapToString(prepareResponses)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java	Using already-accepted recovery for segment starting at txid .*: .*	segmentTxId	bestEntry	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java	Using longest log: .*	bestEntry	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java	None of the responders had a log to recover: .*	QuorumCall.mapToString(prepareResponses)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java	Quorum journal URI '.*' has an even number of Journal Nodes specified. This is not recommended!	uri	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java	Purging remote journals older than txid .*	minTxIdToKeep	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java	Starting recovery process for unclosed journal segments...	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java	Successfully started new epoch .*	loggers.getEpoch()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java	newEpoch(.*) responses:\n.*	loggers.getEpoch()	QuorumCall.mapToString(resps)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/client/QuorumJournalManager.java	selectInputStream manifests:\n.*	Joiner.on("\n").withKeyValueSeparator(": ").join(resps)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/GetJournalEditServlet.java	Received null remoteUser while authorizing access to GetJournalEditServlet	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/GetJournalEditServlet.java	Validating request made by .* / .*. This user is: .*	remotePrincipal	remoteShortName	UserGroupInformation.getLoginUser()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/GetJournalEditServlet.java	SecondaryNameNode principal could not be added	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/GetJournalEditServlet.java	.*	msg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/GetJournalEditServlet.java	isValidRequestor is comparing to valid requestor: .*	v	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/GetJournalEditServlet.java	isValidRequestor is allowing: .*	remotePrincipal	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/GetJournalEditServlet.java	isValidRequestor is allowing other JN principal: .*	remotePrincipal	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/GetJournalEditServlet.java	isValidRequestor is rejecting: .*	remotePrincipal	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/GetJournalEditServlet.java	Received non-NN/JN request for edits from .*	request.getRemoteHost()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/GetJournalEditServlet.java	Received an invalid request file transfer request from .*: .*	request.getRemoteAddr()	msg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JNStorage.java	Purging no-longer needed file .*	txid	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JNStorage.java	Unable to delete no-longer-needed data .*	f	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JNStorage.java	Formatting journal .* with nsid: .*	sd	getNamespaceID()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JNStorage.java	Closing journal storage for .*	sd	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java	Scanning storage .*	fjm	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java	Latest log is .*	latestLog	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java	Latest log .* has no transactions. moving it aside and looking for previous log	latestLog	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java	No files in .*	fjm	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java	Formatting .* with namespace info: .*	this	nsInfo	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java	Updating lastPromisedEpoch from .* to .* for client .*	lastPromisedEpoch.get()	newEpoch	Server.getRemoteIp()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java	Writing txid .*-.*	firstTxnId	lastTxnId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java	Sync of transaction range .*-.* took .*ms	firstTxnId	lastTxnId	milliSeconds	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java	Client is requesting a new log segment .* though we are already writing .*. Aborting the current segment in order to begin the new one.	txid	curSegment	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java	Updating lastWriterEpoch from .* to .* for client .*	curLastWriterEpoch	reqInfo.getEpoch()	Server.getRemoteIp()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java	Validating log segment .* about to be finalized	elf.getFile()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java	Edit log file .* appears to be empty. Moving it aside...	elf	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java	getSegmentInfo(.*): .* -> .*	segmentTxId	elf	TextFormat.shortDebugString(ret)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java	Prepared recovery for segment .*: .*	segmentTxId	TextFormat.shortDebugString(resp)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java	Synchronizing log .*: no current segment in place	TextFormat.shortDebugString(segment)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java	Synchronizing log .*: old segment .* is not the right length	TextFormat.shortDebugString(segment)	TextFormat.shortDebugString(currentSegment)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java	Skipping download of log .*: already have up-to-date logs	TextFormat.shortDebugString(segment)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java	Accepted recovery for segment .*: .*	segmentTxId	TextFormat.shortDebugString(newData)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java	Synchronizing log .* from .*	TextFormat.shortDebugString(segment)	url	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java	Failed to delete temporary file .*	tmpFile	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java	Rolling forward previously half-completed synchronization: .* -> .*	tmp	dst	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java	Starting upgrade of edits directory: .\n   old LV = .*; old CTime = .*.\n   new LV = .*; new CTime = .*	oldLV	oldCTime	storage.getLayoutVersion()	storage.getCTime()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java	Finalizing upgrade for journal .*..*.*; cur CTime = .*	storage.getRoot()	(storage.getLayoutVersion() == 0 ? "" : "\n   cur LV = "	storage.getLayoutVersion()	storage.getCTime())	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNode.java	Initializing journal in directory .*	logDir	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNode.java	Unable to stop HTTP server for .*	this	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/JournalNode.java	Error reported on file .*... exiting	f	new Exception()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/block/BlockTokenSecretManager.java	Exporting access keys	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/block/BlockTokenSecretManager.java	Setting block keys	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/block/BlockTokenSecretManager.java	Updating block keys	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/block/BlockTokenSecretManager.java	Checking access for user=.*, block=.*, access mode=.* using .*	userId	block	mode	id.toString()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/block/BlockTokenSecretManager.java	Generating block token for .*	identifier.toString()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/security/token/delegation/DelegationTokenSecretManager.java	No KEY found for persisted identifier .*	identifier.toString()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java	.* = .* (default=.*)	key	v	defaultValue	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java	.* = .* (default=.*)	key	v	defaultValue	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java	.*[.*] has utilization=.* >= average=.* but it is not specified as a source; skipping it.	dn	t	utilization	average	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java	.* .*: .*	items.size()	name	items	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java	chooseStorageGroups for .*: overUtilized => underUtilized	matcher	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java	chooseStorageGroups for .*: overUtilized => belowAvgUtilized	matcher	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java	chooseStorageGroups for .*: underUtilized => aboveAvgUtilized	matcher	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java	Decided to move .* bytes from .* to .*	StringUtils.byteDesc(size)	source.getDisplayName()	target.getDisplayName()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java	Need to move .* to make the cluster balanced.	StringUtils.byteDesc(bytesLeftToMove)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java	Will move .* in this iteration	StringUtils.byteDesc(bytesBeingMoved)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java	namenodes  = .*	namenodes	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java	parameters = .*	p	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java	included nodes = .*	p.getIncludedNodes()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java	excluded nodes = .*	p.getExcludedNodes()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java	source nodes = .*	p.getSourceNodes()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java	Skipping blockpool .*	nnc.getBlockpoolID()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java	Using a threshold of .*	threshold	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java	Balancer will run on the following blockpools: .*	blockpools.toString()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java	Using a idleiterations of .*	maxIdleIteration	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java	Will run the balancer even during an ongoing HDFS upgrade. Most users will not want to run the balancer during an upgrade since it will not affect used space on over-utilized machines.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Balancer.java	Exiting balancer due an exception	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java	Decided to move .*	this	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java	Start moving .*	this	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java	Successfully moved .*	this	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java	Failed to move .*: .*	this	e.getMessage()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java	.* activateDelay .* seconds	this	delta / 1000.0	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java	getBlocks(.*, .*) returns .* blocks.	getDatanodeInfo()	StringUtils.TraditionalBinaryPrefix.long2String(size,"B",2)	newBlksLocs.getBlocks().length	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java	Add .* to .*	block	this	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java	.* blocksToReceive=.*, scheduledSize=.*, srcBlocks#=.*	this	blocksToReceive	getScheduledSize()	srcBlocks.size()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java	Exception while getting reportedBlock list	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java	Failed to find a pending move .* times.  Skipping .*	noPendingMoveIteration	this	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java	Time up (max time=.* seconds).  Skipping .*	MAX_ITERATION_TIME / 1000	this	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java	Excluding datanode .*: decommissioned=.*, decommissioning=.*, excluded=.*, notIncluded=.*	dn	decommissioned	decommissioning	excluded	notIncluded	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java	No mover threads available: skip moving .*	p	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/Dispatcher.java	Dispatcher thread failed	e.getCause()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/KeyManager.java	Block token params received from NN: update interval=.*, token lifetime=.*	StringUtils.formatTime(updateInterval)	StringUtils.formatTime(tokenLifetime)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/KeyManager.java	Exception shutting down access key updater thread	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/KeyManager.java	Update block keys every .*	StringUtils.formatTime(sleepInterval)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/KeyManager.java	Failed to set keys	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/KeyManager.java	InterruptedException in block key updater thread	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/KeyManager.java	Exception in block key updater thread	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/KeyManager.java	Exception shutting down key updater thread	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/NameNodeConnector.java	No block has been moved for .* iterations, maximum notChangedIterations before exit is: .*	notChangedIterations	((maxNotChangedIterations >= 0) ? maxNotChangedIterations : "Infinite")	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/balancer/NameNodeConnector.java	Failed to delete .*	idPath	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/AvailableSpaceBlockPlacementPolicy.java	Available space block placement policy initialized: .* = .*	DFSConfigKeys.DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY	balancedPreferencePercent	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/AvailableSpaceBlockPlacementPolicy.java	The value of .* is greater than 1.0 but should be in the range 0.0 - 1.0	DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/AvailableSpaceBlockPlacementPolicy.java	The value of .* is less than 0.5 so datanodes with more used percent will receive  more block allocations.	DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	defaultReplication         = .*	defaultReplication	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	maxReplication             = .*	maxReplication	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	minReplication             = .*	minReplication	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	maxReplicationStreams      = .*	maxReplicationStreams	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	replicationRecheckInterval = .*	replicationRecheckInterval	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	encryptDataTransfer        = .*	encryptDataTransfer	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	maxNumBlocksToLog          = .*	maxNumBlocksToLog	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	.*=.*	DFSConfigKeys.DFS_BLOCK_ACCESS_TOKEN_ENABLE_KEY	isEnabled	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	.*=.* min(s), .*=.* min(s), .*=.*	DFSConfigKeys.DFS_BLOCK_ACCESS_KEY_UPDATE_INTERVAL_KEY	updateMin	DFSConfigKeys.DFS_BLOCK_ACCESS_TOKEN_LIFETIME_KEY	lifetimeMin	DFSConfigKeys.DFS_DATA_ENCRYPTION_ALGORITHM_KEY	encryptionAlgorithm	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	Inconsistent number of corrupt replicas for .* blockMap has .* but corrupt replicas map has .*	blk	numCorruptNodes	numCorruptReplicas	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	blocks = .*	java.util.Arrays.asList(blocks)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	Unregistered datanode .*	nodeReg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	Failed to find datanode .*	nodeReg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	Processing first storage report for .* from datanode .*	storageInfo.getStorageID()	nodeID.getDatanodeUuid()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	processReport 0x.*: no zombie storages found.	Long.toHexString(context.getReportId())	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	processReport 0x.*: .* more RPCs remaining in this report.	Long.toHexString(context.getReportId())	(context.getTotalRpcs() - rpcsSeen)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	processReport 0x.*: removing zombie storage .*, which no longer exists on the DataNode.	Long.toHexString(context.getReportId())	zombie.getStorageID()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	processReport 0x.*: removed .* replicas from storage .*, which no longer exists on the DataNode.	Long.toHexString(context.getReportId())	prevBlocks	zombie.getStorageID()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	BLOCK* rescanPostponedMisreplicatedBlocks: Postponed mis-replicated block .* no longer found in block map.	b	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	BLOCK* rescanPostponedMisreplicatedBlocks: Re-scanned block .*, result is .*	b	res	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	Rescan of postponedMisreplicatedBlocks completed in .* msecs. .* blocks are left. .* blocks are removed.	(Time.monotonicNow() - startTimeRescanPostponedMisReplicatedBlocks)	endPostponedMisReplicatedBlocksCount	(startPostponedMisReplicatedBlocksCount - endPostponedMisReplicatedBlocksCount)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	Initial report of block .* on .* size .* replicaState = .*	iblk.getBlockName()	storageInfo.getDatanodeDescriptor()	iblk.getNumBytes()	reportedState	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	Reported block .* on .* size .* replicaState = .*	block	dn	block.getNumBytes()	reportedState	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	In memory blockUCState = .*	ucState	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	Queueing reported block .* in state .* from datanode .* for later processing because .*.	block	reportedState	storageInfo.getDatanodeDescriptor()	reason	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	Processing previouly queued message .*	rbi	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	Processing .* messages from DataNodes that were previously queued during standby state	count	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	Received an RBW replica for .* on .*: ignoring it, since it is complete with the same genstamp	storedBlock	dn	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	.*	msg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	Interrupted while processing replication queues.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	Error while processing replication queues async	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	Interrupted while waiting for replicationQueueInitializer. Returning..	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	block .*: .*	block	res	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	Total number of blocks            = .*	blocksMap.size()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	Number of invalid blocks          = .*	nrInvalid	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	Number of under-replicated blocks = .*	nrUnderReplicated	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	Number of  over-replicated blocks = .*.*.*.*	nrOverReplicated	((nrPostponed > 0) ? (" ("	nrPostponed	" postponed)") : "")	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	Number of blocks being written    = .*	nrUnderConstruction	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	Interrupted while processing replication queues.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	Invalidated .* over-replicated blocks on .* during recommissioning	numOverReplicated	srcNode	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	Node .* hasn't sent its first block report.	node	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	Node .* is dead and there are no under-replicated blocks or blocks pending replication. Safe to decommission.	node	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	Node .* is dead while decommission is in progress. Cannot be safely decommissioned since there is risk of reduced data durability or data loss. Either restart the failed node or force decommissioning by removing, calling refreshNodes, then re-adding to the excludes files.	node	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	BLOCK* .* is not COMPLETE (ucState = .*, replication# = .*.* minimum = .*) in file .*	b	state	numNodes	(numNodes < min ? " < " : " >= ")	min	src	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	In safemode, not computing replication work	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	DataNode .* cannot be found with UUID .*, removing block invalidation work.	dn	dn.getDatanodeUuid()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	Stopping ReplicationMonitor.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	ReplicationMonitor received an exception while shutting down.	t	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	Stopping ReplicationMonitor for testing.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	ReplicationMonitor thread received Runtime exception. 	t	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java	initializing replication queues	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java	Could not find a target for file .* with favored node .*	src	favoredNode	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java	Failed to choose with favored nodes (=.*), disregard favored nodes hint and retry.	favoredNodes	nr	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java	storageTypes=.*	storageTypes	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java	.*	message	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java	.* .*	message	e.getMessage()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java	Failed to choose from local rack (location = .*), retry with the rack of the next replica (location = .*)	localRack	nextNode.getNetworkLocation()	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java	Failed to choose from local rack (location = .*); the second replica is not found, retry choosing ramdomly	localRack	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java	Failed to choose from the next rack (location = .*), retry choosing ramdomly	nextRack	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyDefault.java	Failed to choose remote rack (location = ~.*), fallback to local rack	localMachine.getNetworkLocation()	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyWithNodeGroup.java	Not able to find datanode .* which has dependency with datanode .*	hostname	chosenNode.getHostName()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyWithUpgradeDomain.java	Upgrade domain isn't defined for .*	datanodeInfo	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java	Can't register DN .* because it is already registered.	dn.getDatanodeUuid()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java	Registered DN .* (.*).	dn.getDatanodeUuid()	dn.getXferAddr()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java	Can't unregister DN .* because it is not currently registered.	dn.getDatanodeUuid()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java	DN .* (.*) requested a lease even though it wasn't yet registered.  Registering now.	dn.getDatanodeUuid()	dn.getXferAddr()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java	Removing existing BR lease 0x.* for DN .* in order to issue a new one.	Long.toHexString(node.leaseId)	dn.getDatanodeUuid()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java	Can't create a new BR lease for DN .*, because numPending equals maxPending at .*.  Current leases: .*	dn.getDatanodeUuid()	numPending	allLeases.toString()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java	Created a new BR lease 0x.* for DN .*.  numPending = .*	Long.toHexString(node.leaseId)	dn.getDatanodeUuid()	numPending	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java	Removing expired block report lease 0x.* for DN .*.	Long.toHexString(node.leaseId)	node.datanodeUuid	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java	No entries remaining in the pending list.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java	Datanode .* is using BR lease id 0x0 to bypass rate-limiting.	dn.getDatanodeUuid()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java	BR lease 0x.* is not valid for unknown datanode .*	Long.toHexString(id)	dn.getDatanodeUuid()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java	BR lease 0x.* is not valid for DN .*, because the DN is not in the pending set.	Long.toHexString(id)	dn.getDatanodeUuid()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java	BR lease 0x.* is not valid for DN .*, because the lease has expired.	Long.toHexString(id)	dn.getDatanodeUuid()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java	BR lease 0x.* is not valid for DN .*.  Expected BR lease 0x.*.	Long.toHexString(id)	dn.getDatanodeUuid()	Long.toHexString(node.leaseId)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java	BR lease 0x.* is valid for DN .*.	Long.toHexString(id)	dn.getDatanodeUuid()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java	Can't remove lease for unknown datanode .*	dn.getDatanodeUuid()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java	DN .* has no lease to remove.	dn.getDatanodeUuid()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockReportLeaseManager.java	Removed BR lease 0x.* for DN .*.  numPending = .*	Long.toHexString(id)	dn.getDatanodeUuid()	numPending	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java	Starting CacheReplicationMonitor with interval .* milliseconds	intervalMs	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java	Shutting down CacheReplicationMonitor	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java	Rescanning because of pending operations	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java	Rescanning after .* milliseconds	(curTimeMs - startTimeMs)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java	Scanned .* directive(s) and .* block(s) in .* millisecond(s).	scannedDirectives	scannedBlocks	(curTimeMs - startTimeMs)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java	Shutting down CacheReplicationMonitor.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java	Thread exiting	t	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java	Interrupted while waiting for CacheReplicationMonitor rescan	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java	Directive .*: the directive expired at .* (now = .*)	directive.getId()	directive.getExpiryTime()	now	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java	Directive .*: got UnresolvedLinkException while resolving path .*	directive.getId()	path	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java	Directive .*: No inode found at .*	directive.getId()	path	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java	Directive .*: ignoring non-directive, non-file inode .* 	directive.getId()	node	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java	Directive .*: not scanning file .* because bytesNeeded for pool .* is .*, but the pool's limit is .*	directive.getId()	file.getFullPathName()	pool.getPoolName()	pool.getBytesNeeded()	pool.getLimit()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java	Directive .*: can't cache block .* because it is in state .*, not COMPLETE.	directive.getId()	blockInfo	blockInfo.getBlockUCState()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java	Directive .*: setting replication for block .* to .*	directive.getId()	blockInfo	ocblock.getReplication()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java	Directive .*: caching .*: .*/.* bytes	directive.getId()	file.getFullPathName()	cachedTotal	neededTotal	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java	Block .*: removing from PENDING_UNCACHED for node .* because the DataNode uncached it.	cblock.getBlockId()	datanode.getDatanodeUuid()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java	Block .*: can't cache block because it is .*	cblock.getBlockId()	reason	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java	Block .*: removing from PENDING_CACHED for node .*because we already have .* cached replicas and we only need .*	cblock.getBlockId()	datanode.getDatanodeUuid()	numCached	neededCached	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java	Block .*: removing from PENDING_UNCACHED for node .* because we only have .* cached replicas and we need .*	cblock.getBlockId()	datanode.getDatanodeUuid()	numCached	neededCached	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java	Block .*: removing from cachedBlocks, since neededCached == 0, and pendingUncached and pendingCached are empty.	cblock.getBlockId()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java	Logic error: we're trying to uncache more replicas than actually exist for .*	cachedBlock	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java	Block .*: can't add new cached replicas, because there is no record of this block on the NameNode.	cachedBlock.getBlockId()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java	Block .*: can't cache this block, because it is not yet complete.	cachedBlock.getBlockId()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java	Block .*: DataNode .* is not a valid possibility because the block has size .*, but the DataNode only has .*bytes of cache remaining (.* pending bytes, .* already cached.	blockInfo.getBlockId()	datanode.getDatanodeUuid()	blockInfo.getNumBytes()	pendingCapacity	pendingBytes	datanode.getCacheRemaining()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java	Block .*: added to PENDING_CACHED on DataNode .*	blockInfo.getBlockId()	datanode.getDatanodeUuid()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/CacheReplicationMonitor.java	Block .*: we only have .* of .* cached replicas. .* DataNodes have insufficient cache capacity.	blockInfo.getBlockId()	(cachedBlock.getReplication() - neededCached + chosen.size())	cachedBlock.getReplication()	outOfCapacity	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java	.* had lastBlockReportId 0x.* but curBlockReportId = 0x.*	storageInfo.getStorageID()	Long.toHexString(storageInfo.getLastBlockReportId())	Long.toHexString(curBlockReportId)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java	Number of failed storages changes from .* to .*	this.volumeFailures	volFailures	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java	Number of storages reported in heartbeat=.*; Number of storages in storageMap=.*	reports.length	storageMap.size()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java	.* failed.	storageInfo	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java	Adding new storage ID .* for DN .*	s.getStorageID()	getXferAddr()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java	error reading hosts files: 	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java	.*=.*	DFSConfigKeys.DFS_BLOCK_INVALIDATE_LIMIT_KEY	this.blockInvalidateLimit	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java	.*=.*	DFSConfigKeys.DFS_NAMENODE_DATANODE_REGISTRATION_IP_HOSTNAME_CHECK_KEY	checkIpHostnameInRegistration	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java	The given interval for marking stale datanode = .*, which is less than .* heartbeat intervals. This may cause too frequent changes of stale states of DataNodes since a heartbeat msg may be missing due to temporary short-term failures. Reset stale interval to .*.	staleInterval	DFSConfigKeys.DFS_NAMENODE_STALE_DATANODE_MINIMUM_INTERVAL_DEFAULT	minStaleInterval	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java	The given interval for marking stale datanode = .*, which is larger than heartbeat expire interval .*.	staleInterval	heartbeatExpireInterval	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java	remove datanode .*	nodeInfo	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java	.*.addDatanode: node .* is added to datanodeMap.	getClass().getSimpleName()	node	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java	.*.wipeDatanode(.*): storage .* is removed from datanodeMap.	getClass().getSimpleName()	node	key	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java	Unresolved topology mapping. Using .* for host .*	NetworkTopology.DEFAULT_RACK	node.getHostName()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java	The resolve call returned null!	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java	Unresolved dependency mapping for host .*. Continuing with an empty dependency list	node.getHostName()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java	The dependency call returned null for host .*	node.getHostName()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java	Unresolved datanode registration: .*	message	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java	.*	message	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java	.*	message	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java	Invalid hostname .* in hosts file	hostStr	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java	Skipped stale nodes for recovery : .*	(storages.length - recoveryLocations.size())	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java	Marking all datandoes as stale	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java	Deprecated configuration key .* will be ignored.	deprecatedKey	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java	Please update your configuration to use .* instead.	DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_BLOCKS_PER_INTERVAL_KEY	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java	Activating DecommissionManager with interval .* seconds, .* max blocks per interval, .* max concurrently tracked nodes.	intervalSecs	blocksPerInterval	maxConcurrentTrackedNodes	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java	Starting decommission of .* .* with .* blocks	node	storage	storage.numBlocks()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java	startDecommission: Node .* in .*, nothing to do..*	node	node.getAdminState()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java	stopDecommission: Node .* in .*, nothing to do..*	node	node.getAdminState()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java	Decommissioning complete for node .*	dn	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java	Block .* does not need replication.	block	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java	Block .* numExpected=.*, numLive=.*	block	numExpected	numLive	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java	UC block .* sufficiently-replicated since numLive (.*) >= minR (.*)	block	numLive	blockManager.getMinStorageNum(block)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java	UC block .* insufficiently-replicated since numLive (.*) < minR (.*)	block	numLive	blockManager.getMinStorageNum(block)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java	Block: .*, Expected Replicas: .*, live replicas: .*, corrupt replicas: .*, decommissioned replicas: .*, decommissioning replicas: .*, excess replicas: .*, Is Open File: .*, Datanodes having this block: .*, Current Datanode: .*, Is current datanode decommissioning: .*	block	curExpectedReplicas	curReplicas	num.corruptReplicas()	num.decommissioned()	num.decommissioning()	num.excessReplicas()	bc.isUnderConstruction()	nodeList	srcNode	srcNode.isDecommissionInProgress()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java	Processed .* blocks so far this tick	numBlocksChecked	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java	Namesystem is not running, skipping decommissioning checks.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java	Checked .* blocks and .* nodes this tick	numBlocksChecked	numNodesChecked	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java	Newly-added node .*, doing full scan to find insufficiently-replicated blocks.	dn	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java	Processing decommission-in-progress node .*	dn	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java	Node .* has finished replicating current set of blocks, checking with the full block map.	dn	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java	Node .* is sufficiently replicated and healthy, marked as decommissioned.	dn	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java	.*	b.toString()	dn	blocks.size()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java	Node .* still has .* blocks to replicate before it is a candidate to finish decommissioning.	dn	blocks.size()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DecommissionManager.java	Removing unknown block .*	block	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java	Setting heartbeat recheck interval to .* since .* is less than .*	staleInterval	DFSConfigKeys.DFS_NAMENODE_STALE_DATANODE_INTERVAL_KEY	DFSConfigKeys.DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_KEY	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java	Dead node .* is decommissioned immediately.	node	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java	Stopping decommissioning of .* node .*	node.isAlive() ? "live" : "dead"	node	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java	Exception while checking heartbeat	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java	Skipping next heartbeat scan due to excessive pause	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HostFileManager.java	.*.*	String.format("Failed to resolve address `%s` in `%s`. "	"Ignoring in the %s list.",line,fn,type)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HostFileManager.java	.*Ignoring in .*	String.format("Failed to parse `%s` in `%s`. "	"the %s list.",line,fn,type)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/PendingReplicationBlocks.java	Removing pending replication for .*	block	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/PendingReplicationBlocks.java	PendingReplicationMonitor thread is interrupted.	ie	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/PendingReplicationBlocks.java	PendingReplicationMonitor checking Q	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/PendingReplicationBlocks.java	PendingReplicationMonitor timed out .*	block	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java	getUGI is returning: .*	ugi.getShortUserName()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/MetricsLoggerTask.java	Metrics logging will not be async since the logger is not log4j	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java	Completing previous upgrade for storage directory .*	rootPath	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java	Recovering storage directory .* from previous upgrade	rootPath	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java	Completing previous rollback for storage directory .*	rootPath	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java	Recovering storage directory .* from previous rollback	rootPath	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java	Completing previous finalize for storage directory .*	rootPath	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java	Completing previous checkpoint for storage directory .*	rootPath	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java	Recovering storage directory .* from failed checkpoint	rootPath	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java	Locking is disabled for .*	this.root	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java	.*	msg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java	Unable to acquire file lock on path .*	lockF.toString()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java	Lock on .* acquired by nodename .*	lockF	jvmName	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java	It appears that another node .* has already locked the storage directory: .*	lockingJvmName	root	oe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java	Failed to acquire lock on .*. If this storage directory is mounted via NFS, ensure that the appropriate nfs lock services are running.	lockF	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java	.*	msg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Storage.java	Failed to preserve last modified date from'.*' to '.*'	srcFile	destFile	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Util.java	Syntax error in URI .*. Please check hdfs configuration.	s	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Util.java	Path .* should be specified as a URI in configuration files. Please update hdfs configuration.	s	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/Util.java	Error while processing URI: .*	name	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolManager.java	Removed .*	bpos	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolManager.java	Couldn't remove BPOS .* from bpByNameserviceId map	t	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolManager.java	Refresh request received for nameservices: .*	conf.get(DFSConfigKeys.DFS_NAMESERVICES)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolManager.java	Starting BPOfferServices for nameservices: .*	Joiner.on(",").useForNull("<default>").join(toAdd)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolManager.java	Stopping BPOfferServices for nameservices: .*	Joiner.on(",").useForNull("<default>").join(toRemove)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolManager.java	Refreshing list of NNs for nameservices: .*	Joiner.on(",").useForNull("<default>").join(toRefresh)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java	Block pool storage directory .* does not exist	dataDir	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java	Block pool storage directory .* is not formatted for .*	dataDir	nsInfo.getBlockPoolID()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java	Formatting ...	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java	Failed to analyze storage directories for block pool .*	nsInfo.getBlockPoolID()	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java	Analyzing storage directories for bpid .*	nsInfo.getBlockPoolID()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java	Formatting block pool .* directory .*	blockpoolID	bpSdir.getCurrentDir()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java	Removing block level storage: .*	absPathToRemove	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java	Restored .* block files from trash.	restored	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java	Restored .* block files from trash before the layout upgrade. These blocks will be moved to the previous directory during the upgrade	restored	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java	Upgrading block pool storage directory .*.\n   old LV = .*; old CTime = .*.\n   new LV = .*; new CTime = .*	bpSd.getRoot()	this.getLayoutVersion()	this.getCTime()	HdfsServerConstants.DATANODE_LAYOUT_VERSION	nsInfo.getCTime()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java	Upgrade of block pool .* at .* is complete	blockpoolID	bpSd.getRoot()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java	Not overwriting .* with smaller file from trash directory. This message can be safely ignored.	newChild	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java	Rolling back storage directory .*.\n   target LV = .*; target CTime = .*	bpSd.getRoot()	nsInfo.getLayoutVersion()	nsInfo.getCTime()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java	Rollback of .* is complete	bpSd.getRoot()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java	Finalizing upgrade for storage directory .*.\n   cur LV = .*; cur CTime = .*	dataDirPath	this.getLayoutVersion()	this.getCTime()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java	Finalize upgrade for .* failed.	dataDirPath	ex	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java	Finalize upgrade for .* is complete.	dataDirPath	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java	.*	hardLink.linkStats.report()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java	Restoring .* to .*	blockFile	restoreDirectory	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java	Trash and PreviousDir shouldn't both exist for storage directory .*	sd	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java	Cleared trash for storage directory .*	trashRoot	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java	Created .*	markerFile	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java	.* already exists.	markerFile	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java	Deleting .*	markerFile	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceStorage.java	Failed to delete .*	markerFile	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	.*: .*\n  isClient  =.*, clientname=.*\n  isDatanode=.*, srcDataNode=.*\n  inAddr=.*, myAddr=.*\n  cachingStrategy = .*\n  pinning=.*	getClass().getSimpleName()	block	isClient	clientname	isDatanode	srcDataNode	inAddr	myAddr	cachingStrategy	pinning	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	A packet was last sent .* milliseconds ago.	diff	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	Slow flushOrSync took .*ms (threshold=.*ms), isSync:.*, flushTotalNanos=.*ns	duration	datanodeSlowLogThresholdMs	isSync	flushTotalNanos	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	.*:Exception writing .* to mirror .*	datanode.getDNRegistrationForBP(bpid)	block	mirrorAddr	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	Checksum error in block .* from .*	block	inAddr	ce	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	report corrupt .* from datanode .* to namenode	block	srcDataNode	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	Failed to report bad .* from datanode .* to namenode	block	srcDataNode	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	Receiving one packet for block .*: .*	block	header	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	Slow BlockReceiver write packet to mirror took .*ms (threshold=.*ms)	duration	datanodeSlowLogThresholdMs	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	Receiving an empty packet or the end of the block .*	block	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	receivePacket for .*: previous write did not end at the chunk boundary. onDiskLen=.*	block	onDiskLen	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	Slow BlockReceiver write data to disk cost:.*ms (threshold=.*ms)	duration	datanodeSlowLogThresholdMs	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	Writing out partial crc for data len .*, skip=.*	len	skip	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	Slow manageWriterOsCache took .*ms (threshold=.*ms)	duration	datanodeSlowLogThresholdMs	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	Error managing cache for writer of block .*	block	t	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	Shutting down for restart (.*).	block	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	Exception for .*	block	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	.*\n.*	msg	StringUtils.getStackTrace(responder)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	computePartialChunkCrc for .*: sizePartialChunk=.*, block offset=.*, metafile offset=.*	block	sizePartialChunk	blkoff	ckoff	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	Read in partial CRC chunk from disk for .*	block	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	.*: enqueue .*	myString	p	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	Cannot send OOB response .*. Responder not running.	ackStatus	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	Sending an out of band ack of type .*	ackStatus	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	.*: seqno=.* waiting for local datanode to finish write.	myString	seqno	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	.*: closing	myString	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	.* got .*	myString	ack	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	Relaying an out of band ack of type .*	oobStatus	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	Calculated invalid ack time: .*ns.	ackTimeNanos	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	The downstream error might be due to congestion in upstream including this node. Propagating the error: 	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	.*	myString	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	.*: Thread is interrupted.	myString	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	IOException in BlockReceiver.run(): 	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	.*	myString	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	.*	myString	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	.* terminating	myString	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	Received .* size .* from .*	block	block.getNumBytes()	inAddr	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	Slow PacketResponder send ack to upstream took .*ms (threshold=.*ms), .*, replyAck=.*	duration	datanodeSlowLogThresholdMs	myString	replyAck	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockReceiver.java	.*, replyAck=.*	myString	replyAck	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java	block=.*, (length=.*), syncList=.*	block	block.getNumBytes()	syncList	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java	.* calls recoverBlock(.*, targets=[.*], newGenerationStamp=.*, newBlock=.*, isStriped=.*)	who	block	Joiner.on(", ").join(targets)	rb.getNewGenerationStamp()	rb.getNewBlock()	rb.isStriped()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockRecoveryWorker.java	recoverBlocks FAILED: .*	b	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockScanner.java	Initialized block scanner with targetBytesPerSec .*	this.conf.targetBytesPerSec	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockScanner.java	Disabled block scanner.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockScanner.java	Not adding volume scanner for .*, because the block scanner is disabled.	volume.getBasePath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockScanner.java	Already have a scanner for volume .*.	volume.getBasePath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockScanner.java	Adding scanner for volume .* (StorageID .*)	volume.getBasePath()	volume.getStorageID()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockScanner.java	Not removing volume scanner for .*, because the block scanner is disabled.	volume.getStorageID()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockScanner.java	No scanner found to remove for volumeId .*	volume.getStorageID()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockScanner.java	Removing scanner for volume .* (StorageID .*)	volume.getBasePath()	volume.getStorageID()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockScanner.java	Not scanning suspicious block .* on .*, because the block scanner is disabled.	block	storageId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockScanner.java	Not scanning suspicious block .* on .*, because there is no volume scanner for that storageId.	block	storageId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockScanner.java	Periodic block scanner is not running	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockScanner.java	Returned Servlet info .*	resp	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java	Could not find metadata file for .*	block	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java	.*:sendBlock() : .*	datanode.getDNRegistrationForBP(block.getBlockPoolId())	msg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java	Unable to drop cache on file close	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java	BlockSender.sendChunks() exception: 	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockSender.java	 Could not read or failed to veirfy checksum for data at offset .* for block .*	offset	block	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java	Block pool ID needed, but service not yet registered with NN	new Exception("trace")	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java	Couldn't report bad block .* to .*	block	actor	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java	Namenode .* trying to claim ACTIVE state with txid=.*	actor	txid	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java	NN .* tried to claim ACTIVE state at txid=.* but there was already a more recent claim at txid=.*	actor	txid	lastActiveClaimTxId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java	Acknowledging ACTIVE Namenode .*	actor	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java	Namenode .* taking over ACTIVE state from .* at higher txid=.*	actor	bpServiceToActive	txid	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java	Namenode .* relinquishing ACTIVE state with txid=.*	actor	nnHaState.getTxId()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java	DatanodeCommand action : DNA_REGISTER from .* with .* state	actor.nnAddr	actor.state	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java	DatanodeCommand action: DNA_ACCESSKEYUPDATE	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java	DatanodeCommand action: DNA_BALANCERBANDWIDTHUPDATE	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java	Updating balance throttler bandwidth from .* bytes/s to: .* bytes/s.	dxcs.balanceThrottler.getBandwidth()	bandwidth	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java	DatanodeCommand action: DNA_ERASURE_CODING_RECOVERY	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java	DatanodeCommand action from standby: DNA_ACCESSKEYUPDATE	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java	Got a command from standby NN - ignoring command:.*	cmd.getAction()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java	Unknown DatanodeCommand action: .*	cmd.getAction()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	.* received versionRequest response: .*	this	nsInfo	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	Problem connecting to server: .*	nnAddr	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	Problem connecting to server: .*	nnAddr	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	.*	ive.getMessage()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	Reported NameNode version '.*' does not match DataNode version '.*' but is within acceptable limits. Note: This is normal during a rolling upgrade.	nnVersion	dnVersion	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	.*uccessfully sent block report 0x.*,  containing .* storage report(s), of which we sent .*. The reports had .* total blocks and used .* RPC(s). This took .* msec to generate and .* msecs for RPC and NN processing. Got back .*.* commands: .*.	(success ? "S" : "Uns")	Long.toHexString(reportId)	reports.length	numReportsSent	totalBlockCount	numRPCs	brCreateCost	brSendCost	((nCmds == 0) ? "no commands" : ((nCmds == 1) ? "one command: "	cmds.get(0) : (nCmds	Joiner.on("; ").join(cmds))))	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	Sending cacheReport from service actor: .*	this	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	CacheReport of .* block(s) took .* msec to generate and .* msecs for RPC and NN processing	blockIds.size()	createCost	sendCost	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	Sending heartbeat with .* storage reports from service actor: .*	reports.length	this	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	Invalid BlockPoolId .* in HeartbeatResponse. Expected .*	rollingUpgradeStatus.getBlockPoolId()	bpos.getBlockPoolId()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	For namenode .* using BLOCKREPORT_INTERVAL of .*msec CACHEREPORT_INTERVAL of .*msec Initial delay: .*msec; heartBeatInterval=.*	nnAddr	dnConf.blockReportInterval	dnConf.cacheReportInterval	dnConf.initialBlockReportDelayMs	dnConf.heartBeatInterval	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	.* sent back a full block report lease ID of 0x.*, but we already have a lease ID of 0x.*. Overwriting old lease ID.	nnAddr	Long.toHexString(resp.getFullBlockReportLeaseId())	Long.toHexString(fullBlockReportLeaseId)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	Took .*ms to process .* commands from NN	(endProcessCommands - startProcessCommands)	resp.getCommands().length	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	Forcing a full block report to .*	nnAddr	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	BPOfferService for .* interrupted	this	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	.* is shutting down	this	re	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	RemoteException in offerService	re	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	IOException in offerService	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	.* beginning handshake with NN	this	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	Problem connecting to server: .* :.*	nnAddr	e.getLocalizedMessage()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	Problem connecting to server: .*	nnAddr	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	Block pool .* successfully registered with NN	this	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	BPOfferService .* interrupted while .*	this	stateString	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	.* starting to offer service	this	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	Initialization failed for .* .*	this	ioe.getLocalizedMessage()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	Initialization failed for .*. Exiting. 	this	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	Exception in BPOfferService for .*	this	ex	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	Unexpected exception in block pool .*	this	ex	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	Ending block pool service for: .*	this	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	Error processing datanode Command	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	.*: scheduling an incremental block report.	bpos.toString()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	.*: scheduling a full block report.	bpos.toString()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java	.*.*	baae.getMessage()	nnAddr	baae	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	File descriptor passing is disabled because .*	reason	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	File descriptor passing is enabled.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	.*	this.fileDescriptorPassingDisabledReason	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Configured hostname is .*	hostName	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Reconfiguring .* to .*	property	newVal	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Exception while sending the block report after refreshing volumes .* to .*	property	newVal	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Adding new volumes: .*	Joiner.on(",").join(changedVolumes.newLocations)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Failed to add volume: .*	volume	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Successfully added volume: .*	volume	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Failed to add volume: .*	volume	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Failed to remove volume: .*	e.getMessage()	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	.*	String.format("Deactivating volumes (clear failure=%b): %s",clearFailure,Joiner.on(",").join(absoluteVolumePaths))	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Started plug-in .*	p	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	ServicePlugin .* could not be started	p	t	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Opened IPC server at .*	ipcServer.getListenerAddress()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Periodic Directory Tree Verification scan is disabled because .*	reason	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Opened streaming server at .*	streamingAddr	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Listening on UNIX domain socket: .*	domainPeerServer.getBindPath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Although short-circuit local reads are configured, they are disabled because you didn't configure .*	DFSConfigKeys.DFS_DOMAIN_SOCKET_PATH_KEY	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Cannot find BPOfferService for reporting block received for bpid=.*	block.getBlockPoolId()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Cannot find BPOfferService for reporting block receiving for bpid=.*	block.getBlockPoolId()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Cannot find BPOfferService for reporting block deleted for bpid=.*	block.getBlockPoolId()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Starting DataNode with maxLockedMemory = .*	dnConf.maxLockedMemory	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	dnUserName = .*	dnUserName	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	supergroup = .*	supergroup	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Generated and persisted new Datanode UUID .*	storage.getDatanodeUuid()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Block token params received from NN: for block pool .* keyUpdateInterval=.* min(s), tokenLifetime=.* min(s)	blockPoolId	blockKeyUpdateInterval / (60 * 1000)	blockTokenLifetime / (60 * 1000)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Setting up storage: nsid=.*;bpid=.*;lv=.*;nsInfo=.*;dnuuid=.*	bpStorage.getNamespaceID()	bpid	storage.getLayoutVersion()	nsInfo	storage.getDatanodeUuid()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Connecting to datanode .* addr=.*	dnAddr	addr	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	getBlockLocalPathInfo successful block=.* blockfile .* metafile .*	block	info.getBlockPath()	info.getMetaPath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	getBlockLocalPathInfo for block=.* returning null	block	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	requestShortCircuitFdsForRead failed	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Got: .*	id.toString()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Stopped plug-in .*	p	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	ServicePlugin .* could not be stopped	p	t	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Exception interrupting DataXceiverServer: 	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Exception shutting down DataNode HttpServer	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Waiting for threadgroup to exit, active threads is .*	this.threadGroup.activeCount()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Received exception in BlockPoolManager#shutDownAll: 	ie	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Exception when unlocking storage: .*	ie	ie	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Shutdown complete.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Starting CheckDiskError Thread	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	DataNode.handleDiskError: Keep Running: .*	hasEnoughResources	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	DataNode is shutting down: .*	errMsgr	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	failed to increment network error counts for .*	host	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	.*	msg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	.*	errStr	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	.* Starting thread to transfer .* to .*	bpReg	block	xfersBuilder	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Failed to transfer block .*	blocks[i]	ie	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Connecting to datanode .*	dnAddr	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	.*: Transmitted .* (numBytes=.*) to .*	getClass().getSimpleName()	b	b.getNumBytes()	curTarget	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	.*: close-ack=.*	getClass().getSimpleName()	closeAck	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	.*:Failed to transfer .* to .* got 	bpReg	b	targets[0]	ie	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Cannot find BPOfferService for reporting block received for bpid=.*	block.getBlockPoolId()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Failed to initialize storage directory .*. Exception details: .*	locationString	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Failed to initialize storage directory .*. Exception details: .*	locationString	se	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Received exception in Datanode#join: .*	ex	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Invalid .* .* : 	DFS_DATANODE_DATA_DIR_KEY	location.getFile()	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	-r, --rack arguments are not supported anymore. RackID resolution is handled by the NameNode.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Exception in secureMain	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Exiting Datanode	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Got: .*	id.toString()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	deleteBlockPool command received for block pool .*, force=.*	blockPoolId	force	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	The block pool .* is still running, cannot be deleted.	blockPoolId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	shutdownDatanode command received (upgrade=.*). Shutting down Datanode...	forUpgrade	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Error occurred when removing unhealthy storage dirs: .*	e.getMessage()	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	Unexpected exception occurred while checking disk error  .*	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java	InterruptedException in check disk error thread	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java	Generated new storageID .* for directory .*.*.*	sd.getStorageUuid()	sd.getRoot()	(oldStorageID == null ? "" : (" to replace "	oldStorageID))	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java	Enabled trash for bpid .*	bpid	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java	Cleared trash for bpid .*	bpid	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java	Storage directory .* does not exist	dataDir	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java	Storage directory .* is not formatted for .*	dataDir	nsInfo.getBlockPoolID()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java	Formatting ...	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java	.*.	errorMessage	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java	.*	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java	Storage directory .* has already been used.	dataDir	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java	Failed to add storage for block pool: .* : .*	bpid	e.getMessage()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java	.*	String.format("I/O error attempting to unlock storage directory %s.",sd.getRoot())	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java	DataNode version: .* and NameNode layout version: .*	HdfsServerConstants.DATANODE_LAYOUT_VERSION	nsInfo.getLayoutVersion()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java	Invalid directory in: .*: .*	data.getCanonicalPath()	e.getMessage()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java	Unable to acquire file lock on path .*	oldF.toString()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java	Updating layout version from .* to .* for storage .*	layoutVersion	HdfsServerConstants.DATANODE_LAYOUT_VERSION	sd.getRoot()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java	Upgrading storage directory .*.\n   old LV = .*; old CTime = .*.\n   new LV = .*; new CTime = .*	sd.getRoot()	this.getLayoutVersion()	this.getCTime()	HdfsServerConstants.DATANODE_LAYOUT_VERSION	nsInfo.getCTime()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java	Upgrade of .* is complete	sd.getRoot()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java	Layout version rolled back to .* for storage .*	HdfsServerConstants.DATANODE_LAYOUT_VERSION	sd.getRoot()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java	Rolling back storage directory .*.\n   target LV = .*; target CTime = .*	sd.getRoot()	HdfsServerConstants.DATANODE_LAYOUT_VERSION	nsInfo.getCTime()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java	Rollback of .* is complete	sd.getRoot()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java	Finalizing upgrade for storage directory .*.\n   cur LV = .*; cur CTime = .*	dataDirPath	this.getLayoutVersion()	this.getCTime()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java	Finalize upgrade for .* failed	dataDirPath	ex	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java	Finalize upgrade for .* is complete	dataDirPath	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java	.*	hardLink.linkStats.report()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java	There are .* duplicate block entries within the same volume.	duplicates.size()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java	Unexpectedly low genstamp on .*.	duplicate.src.getAbsolutePath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java	Unexpectedly short length on .*.	duplicate.src.getAbsolutePath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java	Unexpectedly short length on .*.	prevLongest.src.getAbsolutePath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataStorage.java	Discarding .*.	args.src.getAbsolutePath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	Number of active connections is: .*	datanode.getXceiverCount()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	Sending OOB to peer: .*	peer	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	Failed to read expected encryption handshake from client at .*. Perhaps the client is running an older version of Hadoop which does not support encryption	peer.getRemoteAddressString()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	Failed to read expected SASL data transfer protection handshake from client at .*. Perhaps the client is running an older version of Hadoop which does not support SASL data transfer protection	peer.getRemoteAddressString()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	Cached .* closing after .* ops	peer	opsProcessed	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	.*	s	t	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	.*; .*	s	t	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	.*	s1	t	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	.*; .*	s1	t	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	.*	s	t	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	.*:Number of active connections is: .*	datanode.getDisplayName()	datanode.getXceiverCount()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	Reading receipt verification byte for .*	slotId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	Receipt verification is not enabled on the DataNode.  Not verifying .*	slotId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	Unregistering .* because the requestShortCircuitFdsForRead operation failed.	registeredSlotId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	Failed to send success response back to the client.  Shutting down socket for .*.	shmInfo.shmId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	Failed to shut down socket in error handler	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	.*	msg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	Client .* did not send a valid status code after reading. Will close connection.	peer.getRemoteAddressString()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	Error reading client status response. Will close connection.	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	.*:Ignoring exception while serving .* to .*	dnR	block	remoteAddress	ignored	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	.*:Got exception while serving .* to .*	dnR	block	remoteAddress	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	opWriteBlock: stage=.*, clientname=.*\n  block  =.*, newGs=.*, bytesRcvd=[.*, .*]\n  targets=.*; pipelineSize=.*, srcDataNode=.*, pinning=.*	stage	clientname	block	latestGenerationStamp	minBytesRcvd	maxBytesRcvd	Arrays.asList(targets)	pipelineSize	srcDataNode	pinning	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	isDatanode=.*, isClient=.*, isTransfer=.*	isDatanode	isClient	isTransfer	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	writeBlock receive buf size .* tcp no delay .*	peer.getReceiveBufferSize()	peer.getTcpNoDelay()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	Receiving .* src: .* dest: .*	block	remoteAddress	localAddress	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	Connecting to datanode .*	mirrorNode	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	Datanode .* got response for connect ack  from downstream datanode with firstbadlink as .*	targets.length	firstBadLink	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	.*:Exception transfering block .* to mirror .*: .*	datanode	block	mirrorNode	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	.*:Exception transfering .* to mirror .*- continuing without the mirror	datanode	block	mirrorNode	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	Datanode .* forwarding connect ack to upstream firstbadlink is .*	targets.length	firstBadLink	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	TRANSFER: send close-ack	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	Received .* src: .* dest: .* of size .*	block	remoteAddress	localAddress	block.getNumBytes()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	opWriteBlock .* received exception .*	block	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	transferBlock .* received exception .*	blk	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	block=.*, bytesPerCRC=.*, crcPerBlock=.*, md5=.*	block	bytesPerCRC	crcPerBlock	md5	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	blockChecksum .* received exception .*	block	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	Invalid access token in request from .* for OP_COPY_BLOCK for block .* : .*	remoteAddress	block	e.getLocalizedMessage()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	.*	msg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	.*	msg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	Copied .* to .*	block	peer.getRemoteAddressString()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	opCopyBlock .* received exception .*	block	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	Invalid access token in request from .* for OP_REPLACE_BLOCK for block .* : .*	remoteAddress	block	e.getLocalizedMessage()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	.*	msg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	Moved .* from StorageType .* to .*	block	oldReplica.getVolume().getStorageType()	storageType	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	Connecting to datanode .*	dnAddr	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	Moved .* from .*, delHint=.*	block	peer.getRemoteAddressString()	delHint	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	.*	errMsg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	Error writing reply back to .*	peer.getRemoteAddressString()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	Checking block access token for block '.*' with mode '.*'	blk.getBlockId()	mode	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiver.java	Block token verification failed: op=.*, remoteAddress=.*, message=.*	op	remoteAddress	e.getLocalizedMessage()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java	Balancing bandwith is .* bytes/s	bandwidth	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java	Number threads for balancing is .*	maxThreads	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java	.*:DataXceiverServer: 	datanode.getDisplayName()	ace	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java	.*:DataXceiverServer: 	datanode.getDisplayName()	ie	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java	DataNode is out of memory. Will retry in 30 seconds.	ie	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java	.*:DataXceiverServer: Exiting due to: 	datanode.getDisplayName()	te	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java	.* :DataXceiverServer: close exception	datanode.getDisplayName()	ie	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java	Shutting down DataXceiverServer before restart	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java	.*:DataXceiverServer.kill(): 	datanode.getDisplayName()	ie	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java	Got error when sending OOB message.	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java	Interrupted when sending OOB message.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataXceiverServer.java	Closing all peers.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java	.* set to value above 1000 ms/sec. Assuming default value of .*	DFSConfigKeys.DFS_DATANODE_DIRECTORYSCAN_THROTTLE_LIMIT_MS_PER_SEC_KEY	DFSConfigKeys.DFS_DATANODE_DIRECTORYSCAN_THROTTLE_LIMIT_MS_PER_SEC_DEFAULT	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java	.* set to value below 1 ms/sec. Assuming default value of .*	DFSConfigKeys.DFS_DATANODE_DIRECTORYSCAN_THROTTLE_LIMIT_MS_PER_SEC_KEY	DFSConfigKeys.DFS_DATANODE_DIRECTORYSCAN_THROTTLE_LIMIT_MS_PER_SEC_DEFAULT	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java	.*	logMsg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java	this cycle terminating immediately because 'shouldRun' has been deactivated	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java	Exception during DirectoryScanner execution - will continue next cycle	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java	System Error during DirectoryScanner execution - permanently terminating periodic scanner	er	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java	DirectoryScanner: shutdown has been called, but periodic scanner not started	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java	DirectoryScanner: shutdown has been called	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java	interrupted while waiting for masterThread to terminate	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java	interrupted while waiting for reportCompileThreadPool to terminate	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java	.*	statsRecord.toString()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java	Exception occured while compiling report: 	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java	Block: .* has to be upgraded to block ID-based layout	blockId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/ErasureCodingWorker.java	Using striped reads; pool threads=.*	num	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/ErasureCodingWorker.java	Execution for striped reading rejected, Executing in current thread	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/ErasureCodingWorker.java	Using striped block recovery; pool threads=.*	num	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/ErasureCodingWorker.java	Failed to recover striped block .*	recoveryInfo.getExtendedBlock().getLocalBlock()	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/ErasureCodingWorker.java	Failed to recover striped block: .*	blockGroup	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/ErasureCodingWorker.java	Read data interrupted.	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/ErasureCodingWorker.java	.*	e.getMessage()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/ErasureCodingWorker.java	.*	e.getMessage()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/ErasureCodingWorker.java	.*	e.getMessage()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/erasurecode/ErasureCodingWorker.java	.*	e.getMessage()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/AvailableSpaceVolumeChoosingPolicy.java	Available space volume choosing policy initialized: .* = .*, .* = .*	DFS_DATANODE_AVAILABLE_SPACE_VOLUME_CHOOSING_POLICY_BALANCED_SPACE_THRESHOLD_KEY	balancedSpaceThreshold	DFS_DATANODE_AVAILABLE_SPACE_VOLUME_CHOOSING_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY	balancedPreferencePercent	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/AvailableSpaceVolumeChoosingPolicy.java	The value of .* is greater than 1.0 but should be in the range 0.0 - 1.0	DFS_DATANODE_AVAILABLE_SPACE_VOLUME_CHOOSING_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/AvailableSpaceVolumeChoosingPolicy.java	The value of .* is less than 0.5 so volumes with less available disk space will receive more block allocations	DFS_DATANODE_AVAILABLE_SPACE_VOLUME_CHOOSING_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/AvailableSpaceVolumeChoosingPolicy.java	All volumes are within the configured free space balance threshold. Selecting .* for write of block size .*	volume	replicaSize	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/AvailableSpaceVolumeChoosingPolicy.java	Volumes are imbalanced. Selecting .* from high available space volumes for write of block size .*	volume	replicaSize	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/AvailableSpaceVolumeChoosingPolicy.java	Volumes are imbalanced. Selecting .* from low available space volumes for write of block size .*	volume	replicaSize	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java	Failed to mkdirs .*	targetDir	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java	Failed to move meta file from .* to .*	metaFile	targetMetaFile	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java	Failed to move block file from .* to .*	blockFile	targetBlockFile	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java	Failed to move .* to .*	blockFile	targetDir	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java	resolveDuplicateReplicas decide to keep .*.  Will try to delete .*	replicaToKeep	replicaToDelete	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java	Failed to delete block file .*	blockFile	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java	Failed to delete meta file .*	metaFile	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java	Replica Cache file: .* doesn't exist 	replicaFile.getPath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java	Replica Cache file: .* has gone stale	replicaFile.getPath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java	Replica Cache file: .* cannot be deleted	replicaFile.getPath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java	Successfully read replica from cache file : .*	replicaFile.getPath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java	Exception occured while reading the replicas cache file: .*	replicaFile.getPath()	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java	Failed to delete replica cache file: .*	replicaFile.getPath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java	Failed to delete tmp replicas file in .*	tmpFile.getPath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java	Failed to delete replicas file in .*	replicaCacheFile.getPath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java	Failed to write replicas to cache 	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java	Failed to delete replicas file: .*	replicaCacheFile.getPath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/BlockPoolSlice.java	Failed to delete tmp file in .*	tmpFile.getPath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java	AsyncDiskService has already shut down.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java	Shutting down all async disk service threads	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java	All async disk service threads have been shut down	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java	sync_file_range error	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java	Scheduling .* file .* for deletion	block.getLocalBlock()	blockFile	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java	Failed to create trash directory .*	trashDirectory	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java	Moving files .* and .* to trash.	blockFile.getName()	metaFile.getName()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java	Unexpected error trying to .* block .* .* at file .*. Ignored.	(trashDirectory == null ? "delete" : "move")	block.getBlockPoolId()	block.getLocalBlock()	blockFile	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetAsyncDiskService.java	Deleted .* .* file .*	block.getBlockPoolId()	block.getLocalBlock()	blockFile	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java	Block with id .*, pool .* already exists in the FsDatasetCache with state .*	blockId	bpid	prevValue.state	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java	Initiating caching for Block with id .*, pool .*	blockId	bpid	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java	Block with id .*, pool .* does not need to be uncached, because it is not currently in the mappableBlockMap.	blockId	bpid	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java	Cancelling caching for block with id .*, pool .*.	blockId	bpid	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java	.* is anchored, and can't be uncached now.  Scheduling it for uncaching in .* 	key	DurationFormatUtils.formatDurationHMS(revocationPollingMs)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java	.* has been scheduled for immediate uncaching.	key	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java	Block with id .*, pool .* does not need to be uncached, because it is in state .*.	blockId	bpid	prevValue.state	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java	Failed to cache .*: could not reserve .* more bytes in the cache: .* of .* exceeded.	key	length	DFSConfigKeys.DFS_DATANODE_MAX_LOCKED_MEMORY_KEY	maxBytes	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java	Failed to cache .*: Underlying blocks are not backed by files.	key	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java	Failed to cache .*: failed to find backing files.	key	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java	Failed to cache .*: failed to open file	key	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java	Failed to cache .*: checksum verification failed.	key	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java	Failed to cache .*	key	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java	Caching of .* was cancelled.	key	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java	Successfully cached .*.  We are now caching .* bytes in total.	key	newUsedBytes	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java	Caching of .* was aborted.  We are now caching only .* bytes in total.	key	usedBytesCount.get()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java	Uncaching .* now that it is no longer in use by any clients.	key	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java	Forcibly uncaching .* after .* because client(s) .* refused to stop using it.	key	DurationFormatUtils.formatDurationHMS(revocationTimeMs)	dataset.datanode.getShortCircuitRegistry().getClientNames(key)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java	Replica .* still can't be uncached because some clients continue to use it.  Will wait for .*	key	DurationFormatUtils.formatDurationHMS(delta)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java	Uncaching of .* completed. usedBytes = .*	key	newUsedBytes	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetCache.java	Deferred uncaching of .* completed. usedBytes = .*	key	newUsedBytes	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Data node cannot fully support concurrent reading and writing without native code extensions on Windows.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Added volume - .*, StorageType: .*	dir	storageType	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Caught exception when adding .*. Will throw later.	fsVolume	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Added volume - .*, StorageType: .*	dir	storageType	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Removing .* from FsDataset.	absRoot	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Exception thrown while metric collection. Exception : .*	e.getMessage()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	addFinalizedBlock: Moved .* to .* and .* to .*	srcmeta	dstmeta	srcfile	dstfile	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Copied .* to .* and calculated checksum	srcMeta	dstMeta	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Copied .* to .*	srcFile	dstFile	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	truncateBlock: blockFile=.*, metaFile=.*, oldlen=.*, newlen=.*	blockFile	metaFile	oldlen	newlen	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Appending to .*	replicaInfo	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Renaming .* to .*	oldmeta	newmeta	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Renaming .* to .*, file length=.*	blkfile	newBlkFile	blkfile.length()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Cannot move meta file .*back to the finalized directory .*	newmeta	oldmeta	ex	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Recover failed append to .*	b	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Recover failed close .*	b	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Renaming .* to .*	oldmeta	newmeta	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Recover RBW replica .*	b	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Recovering .*	rbw	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Convert .* from Temporary to RBW, visible length=.*	b	visible	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Unable to stop existing writer for block .* after .* miniseconds.	b	writerStopMs	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Changing meta file offset of block .* from .* to .*	b	oldPos	newPos	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Block .* unfinalized and removed. 	b	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	No file exists for block: .*	b	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Not able to delete the block file: .*	blockFile	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Not able to delete the meta block file: .*	metaFile	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	blockId=.*, f=.*	blockId	f	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Failed to delete replica .*: ReplicaInfo not found.	invalidBlks[i]	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Block file .* is to be deleted	removing.getBlockFile().getName()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Volume .* is closed, ignore the deletion task for block .*	v	invalidBlks[i]	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Failed to cache block with id .*, pool .*: ReplicaInfo not found.	blockId	bpid	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Failed to cache block with id .*, pool .*: replica is not finalized; it is in state .*	blockId	bpid	info.getState()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Failed to cache block with id .*, pool .*: volume not found.	blockId	bpid	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Failed to cache block with id .*: volume was not an instance of FsVolumeImpl.	blockId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Caching not supported on block with id .* since the volume is backed by RAM.	blockId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Error registering FSDatasetState MBean	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Registered FSDatasetState MBean	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	FsDatasetImpl.shutdown ignoring InterruptedException from LazyWriter.join	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Deleted a metadata file without a block .*	diskMetaFile.getAbsolutePath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Removed block .* from memory with missing block file on the disk	blockId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Deleted a metadata file for the deleted block .*	diskMetaFile.getAbsolutePath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Added missing block to memory .*	diskBlockInfo	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Failed to delete .*. Will retry on next scan	diskFile	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Block file in volumeMap .* does not exist. Updating it to the file found during scan .*	memFile.getAbsolutePath()	diskFile.getAbsolutePath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Updating generation stamp for block .* from .* to .*	blockId	memBlockInfo.getGenerationStamp()	diskGS	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Metadata file in memory .* does not match file found by scan .*	memMetaFile.getAbsolutePath()	(diskMetaFile == null ? null : diskMetaFile.getAbsolutePath())	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Updating generation stamp for block .* from .* to .*	blockId	memBlockInfo.getGenerationStamp()	gs	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Updating size of block .* from .* to .*	blockId	memBlockInfo.getNumBytes()	memFile.length()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Reporting the block .* as corrupt due to length mismatch	corruptBlock	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Failed to repot bad block .*	corruptBlock	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	initReplicaRecovery: .*, recoveryId=.*, replica=.*	block	recoveryId	replica	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	initReplicaRecovery: update recovery id for .* from .* to .*	block	oldRecoveryID	recoveryId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	initReplicaRecovery: changing replica state for .* from .* to .*	block	replica.getState()	rur.getState()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	updateReplica: .*, recoveryId=.*, length=.*, replica=.*	oldBlock	recoveryId	newlength	replica	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Adding block pool .*	bpid	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Removing block pool .*	bpid	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	.*	e.getMessage()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	.* has some block files, cannot delete unless forced	bpid	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	LazyWriter: Finish persisting RamDisk block:  block pool Id: .* block id: .* to block file .* and meta file .* on target volume .*	bpId	blockId	savedFiles[1]	savedFiles[0]	targetVolume	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Failed to save replica .*. re-enqueueing it.	block	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	LazyWriter: Start persisting RamDisk block: block pool Id: .* block id: .* on target volume .*	block.getBlockPoolId()	block.getBlockId()	targetVolume	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Exception saving replica .*	block	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Failed to save replica .*. re-enqueueing it.	block	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Evicting block .*	replicaState	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	LazyWriter was interrupted, exiting	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Ignoring exception in LazyWriter:	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java	Ignoring exception 	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java	getNextSubDir(.*, .*): no subdirectories found in .*	storageID	bpid	dir.getAbsolutePath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java	getNextSubDir(.*, .*): no more subdirectories found in .*	storageID	bpid	dir.getAbsolutePath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java	getNextSubDir(.*, .*): picking next subdirectory .* within .*	storageID	bpid	nextSubDir	dir.getAbsolutePath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java	getSubdirEntries(.*, .*): purging entries cache for .* after .* ms.	storageID	bpid	state.curFinalizedSubDir	delta	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java	getSubdirEntries(.*, .*): no entries found in .*	storageID	bpid	dir.getAbsolutePath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java	getSubdirEntries(.*, .*): listed .* entries in .*	storageID	bpid	entries.size()	dir.getAbsolutePath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java	nextBlock(.*, .*): advancing from .* to next subdirectory.	storageID	bpid	state.curFinalizedSubDir	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java	nextBlock(.*, .*): advancing to .*	storageID	bpid	block	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java	nextBlock(.*, .*): I/O error	storageID	bpid	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java	save(.*, .*): error deleting temporary file.	storageID	bpid	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java	save(.*, .*): saved .*	storageID	bpid	mapper.writerWithDefaultPrettyPrinter().writeValueAsString(state)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeImpl.java	load(.*, .*): loaded iterator .* from .*: .*	storageID	bpid	name	file.getAbsoluteFile()	mapper.writerWithDefaultPrettyPrinter().writeValueAsString(state)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskAsyncLazyPersistService.java	AsyncLazyPersistService has already shut down.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskAsyncLazyPersistService.java	Shutting down all async lazy persist service threads	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskAsyncLazyPersistService.java	All async lazy persist service threads have been shut down	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskAsyncLazyPersistService.java	LazyWriter schedule async task to persist RamDisk block pool id: .* block id: .*	bpId	blockId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskReplicaTracker.java	Failed to delete block file .*	savedBlockFile	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/RamDiskReplicaTracker.java	Failed to delete meta file .*	savedMetaFile	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ShortCircuitRegistry.java	removing shm .*	shm	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ShortCircuitRegistry.java	created new ShortCircuitRegistry with interruptCheck=.*, shmPath=.*	interruptCheck	shmFactory.getPath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ShortCircuitRegistry.java	Disabling ShortCircuitRegistry	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ShortCircuitRegistry.java	.*	bld.toString()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ShortCircuitRegistry.java	createNewMemorySegment: ShortCircuitRegistry is not enabled.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ShortCircuitRegistry.java	createNewMemorySegment: created .*	info.shmId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ShortCircuitRegistry.java	.* can't register a slot because the ShortCircuitRegistry is not enabled.	this	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ShortCircuitRegistry.java	.*: registered .* with slot .* (isCached=.*)	this	blockId	slotId	isCached	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/ShortCircuitRegistry.java	unregisterSlot: ShortCircuitRegistry is not enabled.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	Starting VolumeScanner .*	scanner.volume.getBasePath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	Successfully scanned .* on .*	block	volume.getBasePath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	Volume .*: block .* is no longer in the dataset.	volume.getBasePath()	block	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	Volume .*: verification failed for .* because of FileNotFoundException.  This may be due to a race with write.	volume.getBasePath()	block	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	Reporting bad .* on .*	block	volume.getBasePath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	Cannot report bad .*	block.getBlockId()	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	unable to instantiate .*	conf.resultHandler	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	.*: error saving .*.	this	iter	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	.*: updateScannedBytes is zeroing out slotIdx .*.  curMinute = .*; newMinute = .*	this	slotIdx	curMinute	newMinute	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	.*: no block pools are registered.	this	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	Now scanning bpid .* on volume .*	iter.getBlockPoolId()	volume.getBasePath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	Now rescanning bpid .* on volume .*, after more than .* hour(s)	iter.getBlockPoolId()	volume.getBasePath()	TimeUnit.HOURS.convert(conf.scanPeriodMs,TimeUnit.MILLISECONDS)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	.*: no suitable block pools found to scan.  Waiting .* ms.	this	minTimeoutMs	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	FileNotFound while finding block .* on volume .*	cblock	volume.getBasePath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	FileNotFoundException while finding block .* on volume .*	cblock	volume.getBasePath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	I/O error while finding block .* on volume .*	cblock	volume.getBasePath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	.*: calculateShouldScan: effectiveBytesPerSec = .*, and targetBytesPerSec = .*.  startMinute = .*, curMinute = .*, shouldScan = .*	storageId	effectiveBytesPerSec	targetBytesPerSec	startMinute	curMinute	shouldScan	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	.*: no block pools are ready to scan yet.  Waiting .* ms.	this	timeout	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	.*: nextBlock error on .*	this	curBlockIter	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	.*: finished scanning block pool .*	this	curBlockIter.getBlockPoolId()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	.*: saving block iterator .* after .* ms.	this	curBlockIter	saveDelta	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	.*: thread starting.	this	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	.* exiting because of InterruptedException.	this	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	.* exiting because of exception 	this	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	.* exiting.	this	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	.*: Not scheduling suspect block .* for rescanning, because this volume scanner is stopping.	this	block	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	.*: Not scheduling suspect block .* for rescanning, because we rescanned it recently.	this	block	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	.*: suspect block .* is already queued for rescanning.	this	block	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	.*: Scheduling suspect block .* for rescanning.	this	block	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	.*: already enabled scanning on block pool .*	this	bpid	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	.*: loaded block iterator for .*.	this	bpid	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	.*: failed to load block iterator: .*	e.getMessage()	this	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	.*: failed to load block iterator.	this	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	.*: created new block iterator for .*.	this	bpid	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	.*: disabling scanning on block pool .*	this	bpid	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java	.*: can't remove block pool .*, because it was never added.	this	bpid	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/DatanodeHttpServer.java	Listening HTTP traffic on .*	httpAddress	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/DatanodeHttpServer.java	Listening HTTPS traffic on .*	httpsAddress	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/SimpleHttpProxyHandler.java	Proxy failed. Cause: 	future.cause()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/SimpleHttpProxyHandler.java	Proxy for .* failed. cause: 	uri	cause	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/SimpleHttpProxyHandler.java	Proxy .* failed. Cause: 	uri	future.cause()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/SimpleHttpProxyHandler.java	Proxy for .* failed. cause: 	uri	cause	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/ExceptionHandler.java	GOT EXCEPTION	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/ExceptionHandler.java	INTERNAL_SERVER_ERROR	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/WebHdfsHandler.java	Error 	cause	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java	Failed to get snapshottable directories. Ignore and continue.	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java	Failed to move some block's after .* retries.	retryMaxAttempts	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java	Failed to list directory .*. Ignore the directory and continue.	fullPath	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java	Failed to check the status of .*. Ignore it and continue.	parent	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java	Failed to get the storage policy of file .*	fullPath	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java	namenodes = .*	namenodes	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java	Exiting .* due to an exception	Mover.class.getSimpleName()	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java	Storage directory .* is not formatted.	sd.getRoot()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java	Formatting ...	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java	Got journal, state = .*; firstTxId = .*; numTxns = .*	bnState	firstTxId	numTxns	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java	data:.*	StringUtils.byteToHexString(data)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java	Loading edits into backupnode to try to catch up from txid .* to .*	lastAppliedTxId	target	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java	Logs rolled while catching up to current segment	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java	Unable to find stream starting with .*. This indicates that there is an error in synchronization in BackupImage	editLog.getCurSegmentTxId()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java	Going to finish converging with remaining .* txns from in-progress stream .*	remainingTxns	stream	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java	Successfully synced BackupNode with NameNode at txnid .*	lastAppliedTxId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java	State transition .* -> .*	bnState	newState	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java	Stopped applying edits to prepare for checkpoint.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java	Waiting until the NameNode rolls its edit logs in order to freeze the BackupNode namespace.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java	Interrupted waiting for namespace to freeze	ie	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupImage.java	BackupNode namespace frozen.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java	Failed to report to name-node.	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java	.*	errorMsg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java	.*	errorMsg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java	Fenced by .* with epoch .*	fencerInfo	epoch	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java	Problem connecting to server: .*	nnAddress	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java	Encountered exception 	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java	Problem connecting to name-node: .*	nnRpcAddress	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java	Encountered exception 	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java	.*	msg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java	.*	errorMsg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java	Using minimum value .* for .*	MIN_CACHED_BLOCKS_PERCENT	DFS_NAMENODE_PATH_BASED_CACHE_BLOCK_MAP_ALLOCATION_PERCENT	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java	Validating directive .* pool maxRelativeExpiryTime .*	info	maxRelativeExpiryTime	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java	addDirective of .* failed: 	info	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java	addDirective of .* successful.	info	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java	modifyDirective of .* failed: 	idString	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java	modifyDirective of .* successfully applied .*.	idString	info	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java	removeDirective of .* failed: 	id	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java	removeDirective of .* successful.	id	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java	addCachePool of .* failed: 	info	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java	addCachePool of .* successful.	info	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java	modifyCachePool of .* failed: 	info	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java	modifyCachePool of .* successful; .*	info.getPoolName()	bld.toString()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java	removeCachePool of .* failed: 	poolName	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java	removeCachePool of .* successful.	poolName	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java	Datanode .* is not a valid cache location for block .* because that node does not have a backing replica!	datanode	block.getBlock().getBlockName()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java	Processed cache report from .*, blocks: .*, processing time: .* msecs	datanodeID	blockIds.size()	(endTime - startTime)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java	Cache report from datanode .* has block .*	datanode	blockId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java	Added block .*  to cachedBlocks	cachedBlock	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java	Added block .* to CACHED list.	cachedBlock	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CacheManager.java	Removed block .* from PENDING_CACHED list.	cachedBlock	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/CheckpointConf.java	Configuration key .* is deprecated! Ignoring... Instead please specify a value for .*	key	DFS_NAMENODE_CHECKPOINT_TXNS_KEY	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/Checkpointer.java	Checkpointer got exception	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/Checkpointer.java	Checkpoint Period : .* secs (.* min)	checkpointConf.getPeriod()	checkpointConf.getPeriod() / 60	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/Checkpointer.java	Transactions count is  : .*, to trigger checkpoint	checkpointConf.getTxnCount()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/Checkpointer.java	Exception in doCheckpoint: 	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/Checkpointer.java	Throwable Exception in doCheckpoint: 	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/Checkpointer.java	Doing checkpoint. Last applied: .*	lastApplied	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/Checkpointer.java	Unable to roll forward using only logs. Downloading image with txid .*	sig.mostRecentCheckpointTxId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/Checkpointer.java	Loading image with txid .*	sig.mostRecentCheckpointTxId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/Checkpointer.java	Checkpoint completed in .* seconds. New Image Size: .*	(monotonicNow() - startTime) / 1000	imageSize	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/Checkpointer.java	Checkpointer about to load edits from .* stream(s).	editsStreams.size()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogBackupOutputStream.java	Nothing to flush	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileInputStream.java	caught exception initializing .*	this	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileInputStream.java	skipping .* bytes at the end of edit log  '.*': reached txid .* out of .*	skipAmt	getName()	txId	lastTxId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileInputStream.java	nextValidOp: got exception while reading .*	this	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileInputStream.java	Log file .* has no valid header	file	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileOutputStream.java	Nothing to flush	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/EditLogFileOutputStream.java	Preallocated .* bytes at the end of the edit log (offset .*)	total	oldSize	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java	Unable to start log segment .* at .*: .*	txid	currentInProgress	e.getLocalizedMessage()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java	Finalizing edits file .* -> .*	inprogressFile	dstFile	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java	Purging logs older than .*	minTxIdToKeep	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java	got IOException while trying to validate header of .*.  Skipping.	elf	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java	Discard the EditLog files, the given start txid is .*	startTxId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java	Trash the EditLog file .*	elf	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java	Edits file .* has improperly formatted transaction ID	f	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java	In-progress edits file .* has improperly formatted transaction ID	f	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java	In-progress stale edits file .* has improperly formatted transaction ID	f	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java	.*: selecting input streams starting at .*.*from among .* candidate file(s)	this	fromTxId	(inProgressOk ? " (inProgress ok) " : " (excluding inProgress) ")	elfs.size()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java	passing over .* because it is in progress and we are ignoring in-progress logs.	elf	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java	got IOException while trying to validate header of .*.  Skipping.	elf	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java	passing over .* because it ends at .*, but we only care about transactions as new as .*	elf	elf.lastTxId	fromTxId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java	selecting edit log stream .*	elf	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java	Recovering unfinalized segments in .*	currentDir	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java	Deleting zero-length edit log file .*	elf	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java	Moving aside edit log file that seems to have zero transactions .*	elf	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java	Starting upgrade of edits directory .*	sd.getRoot()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java	Failed to move aside pre-upgrade storage in image directory .*	sd.getRoot()	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java	ACLs enabled? .*	aclsEnabled	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java	XAttrs enabled? .*	xattrsEnabled	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java	Initializing quota with .* thread(s)	threads	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java	Quota initialization completed in .* milliseconds\n.*	(Time.now() - start)	counts	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java	Namespace quota violation in image for .* quota = .* < consumed = .*	dir.getFullPathName()	nsQuota	nsConsumed	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java	Storagespace quota violation in image for .* quota = .* < consumed = .*	dir.getFullPathName()	ssQuota	ssConsumed	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java	Storage type quota violation in image for .* type = .* quota = .* < consumed .*	dir.getFullPathName()	t.toString()	typeQuota	typeSpace	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java	Setting quota for .*\n.*	dir	myCounts	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java	Initializing shared journals for READ, already open for READ	new Exception()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java	No edits directories configured!	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java	Closing log when already closed	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java	Error closing journalSet	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java	.*	msg	new Exception()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java	.*	msg	new Exception()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java	.*	buf	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java	Rolling edit logs	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java	Started a new log segment at txid .*	txid	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java	.*	mess	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java	Starting log segment at .*	segmentTxId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java	Ending log segment .*	curSegmentTxId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java	All journals failed to abort	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java	Backup node .* re-registers	bnReg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java	Registering new backup node: .*	bnReg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java	Removing backup journal .*	bjm	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java	.*	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java	No class configured for .*, .* is empty	uriScheme	key	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java	Acquiring write lock to replay edit log	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java	op=.*, startOpt=.*, numEdits=.*, totalEdits=.*	op	startOpt	numEdits	totalEdits	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java	Encountered exception on operation .*	op	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java	replaying edit log: .*/.* transactions completed. (.*.*	deltaTxId	numTxns	percent	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java	Stopped at OP_START_ROLLING_UPGRADE for rollback.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java	replaying edit log finished	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLogLoader.java	replaying edit log: .*	op	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java	Allocated new BlockPoolId: .*	ns.getBlockPoolID()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java	Data dir states:\n  .*	Joiner.on("\n  ").withKeyValueSeparator(": ").join(dataDirStates)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java	Storage directory .* is not formatted.	sd.getRoot()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java	Formatting ...	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java	Starting upgrade of local storage directories.\n   old LV = .*; old CTime = .*.\n   new LV = .*; new CTime = .*	oldLV	oldCTime	storage.getLayoutVersion()	storage.getCTime()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java	Failed to move aside pre-upgrade storage in image directory .*	sd.getRoot()	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java	Can perform rollback for .*	sd	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java	Can perform rollback for shared edit log.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java	Rolling back storage directory .*.\n   new LV = .*; new CTime = .*	sd.getRoot()	prevState.getStorage().getLayoutVersion()	prevState.getStorage().getCTime()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java	Finalizing upgrade for local dirs. .*.*; cur CTime = .*	(storage.getLayoutVersion() == 0 ? "" : "\n   cur LV = "	storage.getLayoutVersion()	storage.getCTime())	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java	Reloading namespace from .*	file	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java	Planning to load edit log stream: .*	l	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java	No edit log streams selected.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java	Failed to load image from .*	imageFile	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java	Planning to load image :\n.*	imageFile	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java	About to load edits:\n  .*	Joiner.on("\n  ").join(editStreams)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java	Reading .* expecting start txid #.*.*	editIn	(lastAppliedTxId	1)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java	Loaded image for txid .* from .*	txId	curFile	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java	Cancelled image saving for .*: .*	sd.getRoot()	snce.getMessage()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java	Unable to save image for .*	sd.getRoot()	t	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java	Caught interrupted exception while waiting for thread .* to finish. Retrying join	thread.getName()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java	Unable to purge old storage .*	nnf.getName()	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java	Unable to rename checkpoint in .*	sd	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java	Unable to rename checkpoint in .*	image.sd	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java	Unable to delete cancelled checkpoint in .*	sd	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java	renaming  .* to .*	fromFile.getAbsolutePath()	toFile.getAbsolutePath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java	Start checkpoint at txid .*	getEditLog().getLastWrittenTxId()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java	.*	msg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java	End checkpoint at txid .*	getEditLog().getLastWrittenTxId()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java	Renaming reserved path .* to .*	oldPath	newPath	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java	Number of files under construction = .*	size	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java	Will rename reserved path .* to .*	key	value	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java	Upgrade process renamed reserved path .* to .*	oldPath	path	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java	Renamed root path .* to .*	FSDirectory.DOT_RESERVED_STRING	renameString	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java	Saving image file .* using .*	newFile	compression	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormat.java	Image file .* of size .* bytes saved in .* seconds.	newFile	newFile.length()	(monotonicNow() - startTime) / 1000	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java	Loading .* INodes.	numInodes	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java	Fail to find inode .* when saving the leases.	id	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatPBINode.java	Fail to save the lease for inode id .* as the file is not under construction	id	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java	Loaded FSImage in .* seconds.	(end - start) / 1000	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageFormatProtobuf.java	Unrecognized section .*	n	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImagePreTransactionalStorageInspector.java	This is a rare failure scenario!!!	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImagePreTransactionalStorageInspector.java	Image checkpoint time .* > edits checkpoint time .*	latestNameCheckpointTime	latestEditsCheckpointTime	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImagePreTransactionalStorageInspector.java	Name-node will treat the image as the latest state of the namespace. Old edits will be discarded.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImagePreTransactionalStorageInspector.java	Performing recovery in .* and .*	latestNameSD	latestEditsSD	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImagePreTransactionalStorageInspector.java	Unable to delete dir .* before rename	curFile	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImagePreTransactionalStorageInspector.java	Name checkpoint time is newer than edits, not loading edits.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageTransactionalStorageInspector.java	No version file in .*	sd.getRoot()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageTransactionalStorageInspector.java	Unable to determine the max transaction ID seen by .*	sd	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageTransactionalStorageInspector.java	Unable to inspect storage directory .*	currentDir	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageTransactionalStorageInspector.java	Checking file .*	f	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageTransactionalStorageInspector.java	Image file .* has improperly formatted transaction ID	f	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImageTransactionalStorageInspector.java	Found image file at .* but storage directory is not configured to contain images.	f	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Only one image storage directory (.*) configured. Beware of data loss due to lack of redundant storage directories!	DFS_NAMENODE_NAME_DIR_KEY	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Only one namespace edits storage directory (.*) configured. Beware of data loss due to lack of redundant storage directories!	DFS_NAMENODE_EDITS_DIR_KEY	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Encountered exception loading fsimage	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Finished loading FSImage in .* msecs	timeTakenToLoadFSImage	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	No KeyProvider found.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Found KeyProvider: .*	provider.toString()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Enabling async auditlog	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	fsLock is fair:.*	fair	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	fsOwner             = .*	fsOwner	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	supergroup          = .*	supergroup	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	isPermissionEnabled = .*	isPermissionEnabled	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Determined nameservice ID: .*	nameserviceId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	HA Enabled: .*	haEnabled	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Configured NNs:\n.*	DFSUtil.nnAddressesAsString(conf)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Using INode attribute provider: .*	klass.getName()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	.* initialization failed.	getClass().getSimpleName()	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	.* initialization failed.	getClass().getSimpleName()	re	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Retry cache on namenode is .*	(enable ? "enabled" : "disabled")	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Retry cache will use .* of total heap and retry cache entry expiry time is .* millis	heapPercent	entryExpiryMillis	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Need to save fs image? .* (staleImage=.*, haEnabled=.*, isRollingUpgrade=.*)	needToSave	staleImage	haEnabled	isRollingUpgrade()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Starting services required for active state	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Catching up to latest edits from old active before taking over writer role in edits logs	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Reprocessing replication and invalidation queues	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	NameNode metadata after re-processing replication and invalidation queues during failover:\n.*	metaSaveAsString()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Will take over writing edit logs at txnid .*	nextTxId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Lazy persist file scrubber is disabled, configured scrub interval is zero.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Stopping services started for active state	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Starting services required for standby state	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Stopping services started for standby state	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	!!! WARNING !!!\n\tThe NameNode currently runs without persistent storage.\n\tAny changes to the file system meta-data may be lost.\n\tRecommended actions:.*.*.*\n\t\t- use Backup Node as a persistent and up-to-date storage of the file system meta-data.	"\n\t\t- shutdown and restart NameNode with configured \""	propertyName	"\" in hdfs-site.xml;"	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Edits URI .* listed multiple times in .*. Ignoring duplicates.	dir	DFS_NAMENODE_SHARED_EDITS_DIR_KEY	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Edits URI .* listed multiple times in .* and .*. Ignoring duplicates.	dir	DFS_NAMENODE_SHARED_EDITS_DIR_KEY	DFS_NAMENODE_EDITS_DIR_KEY	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	FSNamesystem write lock held for .* ms via\n.*	writeLockInterval	StringUtils.getStackTrace(Thread.currentThread())	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Failed to update the access time of .*	src	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Ignoring unknown CryptoProtocolVersion provided by client: .*	c.getUnknownValue()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	recoverLease: .*, src=.* from client .*	lease	src	clientName	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	startFile: recover .*, src=.* client .*	lease	src	clientName	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Adjusting safe-mode totals for deletion.decreasing safeBlocks by .*, totalBlocks by .*	numRemovedSafe	numRemovedComplete	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Error while resolving the link : .*	fullName	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	commitBlockSynchronization(oldBlock=.*, newgenerationstamp=.*, newlength=.*, newtargets=.*, closeFile=.*, deleteBlock=.*)	oldBlock	newgenerationstamp	newlength	Arrays.asList(newtargets)	closeFile	deleteblock	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Block (=.*) not found	oldBlock	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Unexpected block (=.*) since the file (=.*) is not under construction	oldBlock	iFile.getLocalName()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	DatanodeDescriptor (=.*) not found	newtargets[i]	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	commitBlockSynchronization(oldBlock=.*, file=.*.*.*.*, newlength=.*, newtargets=.*) successful	oldBlock	src	(copyTruncate ? ", newBlock="	truncatedBlock : ", newgenerationstamp="	newgenerationstamp)	newlength	Arrays.asList(newtargets)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	commitBlockSynchronization(.*) successful	oldBlock	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	.*Entering safe mode.	lowResourcesMsg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	.*Already in safe mode.	lowResourcesMsg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	New namespace image has been created	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	The threshold value should't be greater than 1, threshold: .*	threshold	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	.* = .*	DFS_NAMENODE_SAFEMODE_THRESHOLD_PCT_KEY	threshold	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	.* = .*	DFS_NAMENODE_SAFEMODE_MIN_DATANODES_KEY	datanodeThreshold	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	.*     = .*	DFS_NAMENODE_SAFEMODE_EXTENSION_KEY	extension	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Refusing to leave safe mode without a force flag. Exiting safe mode will cause a deletion of .* byte(s). Please use -forceExit flag to exit safe mode forcefully if data loss is acceptable.	blockManager.getBytesInFuture()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Adjusting block totals from .*/.* to .*.*/.*.*	blockSafe	blockTotal	(blockSafe	deltaSafe)	(blockTotal	deltaTotal)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	NameNode is being shutdown, exit SafeModeMonitor thread	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Refusing to leave safe mode without a force flag. Exiting safe mode will cause a deletion of .* byte(s). Please use -forceExit flag to exit safe mode forcefully and data loss is acceptable.	blockManager.getBytesInFuture()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Leaving safe mode due to forceExit. This will cause a data loss of .* byte(s).	blockManager.getBytesInFuture()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	forceExit used when normal exist would suffice. Treating force exit as normal safe mode exit.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Unexpected safe mode action	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Roll Edit Log from .*	Server.getRemoteAddress()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Start checkpoint for .*	backupNode.getAddress()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	End checkpoint for .*	registration.getAddress()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Registered FSNamesystemState MBean	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Failed to fetch TopUser metrics	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	updatePipeline(.*, newGS=.*, newLength=.*, newNodes=.*, client=.*)	oldBlock.getLocalBlock()	newBlock.getGenerationStamp()	newBlock.getNumBytes()	Arrays.asList(newNodes)	clientName	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	updatePipeline(.* => .*) success	oldBlock.getLocalBlock()	newBlock.getLocalBlock()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	.*	msg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	.*	msg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	there are no corrupt file blocks.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	list corrupt file blocks returned: .*	count	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	trying to get DT with no secret manager running	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Get corrupt file blocks returned error: .*	e.getMessage()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Successfully saved namespace for preparing rolling upgrade.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Encountered exception setting Rollback Image	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java	Log4j is required to enable async auditlog	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSPermissionChecker.java	ACCESS CHECK: .*, doCheckOwner=.*, ancestorAccess=.*, parentAccess=.*, access=.*, subAccess=.*, ignoreEmptyDir=.*	this	doCheckOwner	ancestorAccess	parentAccess	access	subAccess	ignoreEmptyDir	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/BootstrapStandby.java	Unable to fetch namespace information from remote NN at .*: .*	otherIpcAddress	ioe.getMessage()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/BootstrapStandby.java	Full exception trace	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/BootstrapStandby.java	Unable to fetch namespace information from any remote NN. Possible NameNodes: .*	remoteNNs	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/BootstrapStandby.java	Layout version on remote node (.*) does not match this node's layout version (.*)	nsInfo.getLayoutVersion()	HdfsServerConstants.NAMENODE_LAYOUT_VERSION	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/BootstrapStandby.java	The active NameNode is in Upgrade. Prepare the upgrade for the standby NameNode as well.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/BootstrapStandby.java	The storage directory is in an inconsistent state	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/BootstrapStandby.java	Failed to move aside pre-upgrade storage in image directory .*	sd.getRoot()	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/BootstrapStandby.java	Found nn: .*, ipc: .*	info.getNameNodeID()	info.getIpcAddress()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/BootstrapStandby.java	Could not determine valid IPC address for other NameNode (.*) , got: .*	info.getNameNodeID()	address	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/ConfiguredFailoverProxyProvider.java	Failed to create RPC proxy to NameNode	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java	Will roll logs on active node every .* seconds.	(logRollPeriodMs / 1000)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java	Not going to trigger log rolls on active node because .* is negative.	DFSConfigKeys.DFS_HA_LOGROLL_PERIOD_KEY	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java	Specified a non-positive number of retries for the number of retries for the namenode connection when manipulating the edit log (.*), setting to default: .*	DFSConfigKeys.DFS_HA_TAILEDITS_ALL_NAMESNODES_RETRY_KEY	DFSConfigKeys.DFS_HA_TAILEDITS_ALL_NAMESNODES_RETRY_DEFAULT	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java	logRollPeriodMs=.* sleepTime=.*	logRollPeriodMs	sleepTimeMs	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java	Edit log tailer thread exited with an exception	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java	lastTxnId: .*	lastTxnId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java	Edits tailer failed to find any streams. Will try again later.	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java	edit streams to load from: .*	streams.size()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java	.*	String.format("Loaded %d edits starting from txid %d ",editsLoaded,lastTxnId)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java	Triggering log roll on remote NameNode	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java	Skipping log roll. Remote node is not in Active state: .*	ioe.getMessage().split("\n")[0]	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java	Unable to trigger a roll of the active NN	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java	Error while reading edits from disk. Will try again.	elie	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java	Unknown error encountered while tailing edits. Shutting down standby NN.	t	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java	Edit log tailer interrupted	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java	Failed to reach remote node: .*, retrying with remaining remote NNs	currentNN	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/EditLogTailer.java	Failed to reach .*	currentNN	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java	Starting standby checkpoint thread...\nCheckpointing active NN to possible NNs: .*\nServing checkpoints at .*	activeNNAddresses	myNNAddress	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java	Edit log tailer thread exited with an exception	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java	A checkpoint was triggered but the Standby Node has not received any transactions since the last checkpoint at txid .*. Skipping...	thisCheckpointTxId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java	Triggering a rollback fsimage for rolling upgrade.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java	Triggering checkpoint because there have been .* txns since the last checkpoint, which exceeds the configured threshold .*	uncheckpointed	checkpointConf.getTxnCount()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java	Triggering checkpoint because it has been .* seconds since the last checkpoint, which exceeds the configured interval .*	secsSinceLast	checkpointConf.getPeriod()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java	But skipping this checkpoint since we are about to failover!	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java	Checkpoint was cancelled: .*	ce.getMessage()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java	Interrupted during checkpointing	ie	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ha/StandbyCheckpointer.java	Exception in doCheckpoint	t	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ImageServlet.java	Received non-NN/SNN/administrator request for image or edits from .* at .*	request.getUserPrincipal().getName()	request.getRemoteHost()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ImageServlet.java	Received an invalid request file transfer request from a secondary with storage info .*	theirStorageInfoString	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ImageServlet.java	Received null remoteUser while authorizing access to getImage servlet	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ImageServlet.java	SecondaryNameNode principal could not be added	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ImageServlet.java	.*	msg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ImageServlet.java	ImageServlet allowing checkpointer: .*	remoteUser	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ImageServlet.java	ImageServlet allowing administrator: .*	remoteUser	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/ImageServlet.java	ImageServlet rejecting: .*	remoteUser	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodesInPath.java	UnresolvedPathException  path: .* preceding: .* count: .* link: .* target: .* remainder: .*	path	preceding	count	link	target	remainder	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalSet.java	Unable to abort stream .*	stream	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalSet.java	Skipping jas .* since it's disabled	jas	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalSet.java	Unable to determine input streams from .*. Skipping.	jas.getManager()	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalSet.java	Disabling journal .*	j	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalSet.java	.*	msg	t	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalSet.java	Error: .* failed for (journal .*)	status	jas	t	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalSet.java	Error: .*	message	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalSet.java	Error in setting outputbuffer capacity	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalSet.java	Cannot list edit logs in .*	fjm	t	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalSet.java	Found gap in logs at .*: not returning previous logs in manifest.	curStartTxId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/JournalSet.java	Generated manifest for logs since .*:.*	fromTxId	ret	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java	inode .* not found in lease.files (=.*)	inodeId	lease	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java	.* not found in sortedLeases	lease	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java	Removing non-existent lease! holder=.* src=.*	holder	src.getFullPathName()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java	.* is interrupted	name	ie	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java	Unexpected throwable: 	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java	.* has expired hard limit	leaseToCheck	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java	Lease recovery for inode .* is complete. File closed.	id	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java	Started block recovery .* lease .*	p	leaseToCheck	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java	Cannot release the path .* in the lease .*	p	leaseToCheck	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java	Encountered exception 	ie	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/MetaRecoveryContext.java	.*	prompt	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/MetaRecoveryContext.java	Continuing	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/MetaRecoveryContext.java	Exiting on user request.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameCache.java	initialized with .* entries .* lookups	size()	lookups	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java	.* is .*	FS_DEFAULT_NAME_KEY	nnAddr	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java	Clients are to use .* to access this namenode/service.	clientNamenodeAddress	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java	Setting ADDRESS .*	address	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java	Setting lifeline RPC address .*	lifelineRPCAddress	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java	Clients are to use .* to access this namenode/service.	clientNamenodeAddress	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java	ServicePlugin .* could not be started	p	t	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java	.* RPC up at: .*	getRole()	rpcServer.getRpcAddress()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java	.* service RPC up at: .*	getRole()	rpcServer.getServiceRpcAddress()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java	ServicePlugin .* could not be stopped	p	t	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java	Exception while stopping httpserver	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java	Caught interrupted exception 	ie	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java	Encountered exception while exiting state 	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java	Encountered exception during format: 	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java	No shared edits directory configured for namespace .* namenode .*	nsId	namenodeId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java	Could not initialize shared edits dir	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java	Could not close sharedEditsImage	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java	Could not unlock storage directories	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java	Beginning to copy stream .* to shared edits	stream	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java	copying op: .*	op	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java	ending log segment because of END_LOG_SEGMENT op in .*	stream	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java	ending log segment because of end of stream in .*	stream	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java	Must specify a valid cluster ID after the .* flag	StartupOption.CLUSTERID.getName()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java	Must specify a valid cluster ID after the .* flag	StartupOption.CLUSTERID.getName()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java	Must specify a valid cluster ID after the .* flag	StartupOption.CLUSTERID.getName()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java	Unknown upgrade flag .*	flag	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java	Must specify a rolling upgrade startup option .*	RollingUpgradeStartupOption.getAllOptionString()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java	Invalid argument: .*	args[i]	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java	createNameNode .*	Arrays.asList(argv)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java	Setting .* to .*	FS_DEFAULT_NAME_KEY	defaultUri.toString()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java	Failed to start namenode.	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java	.*	message	t	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java	Allowing manual HA control from .* even though automatic HA is enabled, because the user specified the force flag	Server.getRemoteAddress()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java	Block .* .*	blockId	NONEXISTENT_STATUS	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java	.*	errMsg	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java	Error in looking up block	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java	.*	sb.toString()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java	.*	msg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java	.*	errMsg	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java	Fsck: ignoring open file .*	path	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java	Fsck: deleted corrupt file .*	path	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java	Fsck: error deleting corrupted file .*	path	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java	Fsck: can't copy the remains of .* to .*.*.* already exists.	fullName	"lost	found, because "	target	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java	Fsck: could not copy block .* to .*	lblock.getBlock()	target	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java	Fsck: there were errors copying the remains of the corrupted file .*.*.*	fullName	" to /lost	found"	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java	Fsck: copied the remains of the corrupted file .*.*.*	fullName	" to /lost	found"	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java	copyBlocksToLostFound: error processing .*	fullName	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java	Could not obtain block from any node:  .*	ie	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java	Failed to connect to .*:.*	targetAddr	ex	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java	Error reading block	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java	.*.*	"Cannot use /lost	found : a regular file with this name exists."	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java	.*.*	"Cannot initialize /lost	found ."	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeResourceChecker.java	Space available on volume '.*' is .*	volume	availableSpace	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeResourceChecker.java	Space available on volume '.*' is .*, which is below the configured reserved amount .*	volume	availableSpace	duReserved	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeResourceChecker.java	Going to check the following volumes disk space: .*	volumes	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java	Service RPC server is binding to .*:.*	bindHost	serviceRpcAddr.getPort()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java	Lifeline RPC server is binding to .*:.*	bindHost	lifelineRpcAddr.getPort()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java	RPC server is binding to .*:.*	bindHost	rpcAddr.getPort()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java	Error report from .*: .*	registration	msg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java	getAdditionalDatanode: src=.*, fileId=.*, blk=.*, existings=.*, excludes=.*, numAdditionalNodes=.*, clientName=.*	src	fileId	blk	Arrays.asList(existings)	Arrays.asList(excludes)	numAdditionalNodes	clientName	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java	rollingUpgrade .*	action	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java	Error report from .*: .*	dnName	msg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java	Disk error on .*: .*	dnName	msg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java	Fatal disk error on .*: .*	dnName	msg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java	Error report from .*: .*	dnName	msg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java	Registration IDs mismatched: the .* ID is .* but the expected ID is .*	nodeReg.getClass().getSimpleName()	id	expectedID	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java	Refreshing all user-to-groups mappings. Requested by user: .*	getRemoteUser().getShortUserName()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java	Refreshing SuperUser proxy group mapping list 	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java	Refreshing call queue.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java	Getting groups for user .*	user	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java	.* DN: .*	ive.getMessage()	dnReg	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java	.*	ive.toString()	ive	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java	.*. Note: This is normal during a rolling upgrade.	messagePrefix	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java	Tried to read from deleted or moved edit log segment	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java	Tried to read from deleted edit log segment	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java	NN is transitioning from active to standby and FSEditLog is closed -- could not read edits	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java	set restore failed storage to .*	val	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java	NNStorage.attemptRestoreRemovedStorage: check removed(failed) storarge. removedStorages size = .*	removedStorageDirs.size()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java	currently disabled dir .*; type=.*;canwrite=.*	root.getAbsolutePath()	sd.getStorageDirType()	FileUtil.canWrite(root)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java	restoring dir .*	sd.getRoot().getAbsolutePath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java	Error converting file to URI	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java	writeTransactionIdToStorage failed on .*	sd	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java	Storage directory .* has been successfully formatted.	sd.getRoot()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java	Error reported on storage directory .*	sd	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java	current list of storage dirs:.*	lsd	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java	About to remove corresponding storage: .*	sd.getRoot().getAbsolutePath()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java	Unable to unlock bad storage directory: .*	sd.getRoot().getPath()	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java	at the end current list of storage dirs:.*	lsd	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java	Clusterid mismatch - current clusterid: .*, Ignoring given clusterid: .*	getClusterID()	startOpt.getClusterId()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java	Using clusterid: .*	getClusterID()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java	current cluster id for sd=.*;lv=.*;cid=.*	sd.getCurrentDir()	layoutVersion	cid	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java	this sd not available: .*	e.getLocalizedMessage()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java	couldn't find any VERSION file containing valid ClusterId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorage.java	.*	"Could not find ip address of \"default\" inteface."	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java	Going to retain .* images with txid >= .*	toRetain	minTxId	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java	Purging old edit log .*	log	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java	Purging old image .*	image	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java	Could not delete .*	file	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java	Invalid file name. Skipping .*	fName	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java	Deleting .*	fileName	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNStorageRetentionManager.java	Failed to delete image file: .*	fileToDelete	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNUpgradeUtil.java	Storage directory .* does not contain previous fs state.	sd.getRoot()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNUpgradeUtil.java	Directory .* does not exist.	prevDir	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNUpgradeUtil.java	Finalize upgrade for .* is not required.	sd.getRoot()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNUpgradeUtil.java	Finalizing upgrade of storage directory .*	sd.getRoot()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNUpgradeUtil.java	Finalize upgrade for .* is complete.	sd.getRoot()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNUpgradeUtil.java	Starting upgrade of storage directory .*	sd.getRoot()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNUpgradeUtil.java	Performing upgrade of storage directory .*	sd.getRoot()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNUpgradeUtil.java	Unable to rename temp to previous for .*	sd.getRoot()	ioe	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NNUpgradeUtil.java	Rollback of .* is complete.	sd.getRoot()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/RedundantEditLogInputStream.java	Fast-forwarding stream '.*' to transaction ID .*.*	streams[curIdx].getName()	(prevTxId	1)	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/RedundantEditLogInputStream.java	Got error reading edit log input stream .*; failing over to edit log .*.*	streams[curIdx].getName()	streams[curIdx	1].getName()	prevException	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/RedundantEditLogInputStream.java	failing over to edit log .*.*	streams[curIdx	1].getName()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java	Checkpoint Period   :.* secs (.* min)	checkpointConf.getPeriod()	checkpointConf.getPeriod() / 60	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java	Log Size Trigger    :.* txns	checkpointConf.getTxnCount()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java	Exception 	ie	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java	Interrupted waiting to join on checkpointer thread	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java	Exception shutting down SecondaryNameNode	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java	Exception while closing CheckpointStorage	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java	Exception in doCheckpoint	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java	Merging failed .* times.	checkpointImage.getMergeErrorCount()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java	Throwable Exception in doCheckpoint	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java	Image has not changed. Will not download image.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java	Image has changed. Downloading updated image from NN.	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java	Will connect to NameNode at .*	address	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java	Web server init done	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java	Checkpoint done. New Image Size: .*	dstStorage.getFsImageName(txid).length()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java	Failed to write legacy OIV image: 	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java	.*: .*	cmd	content[0]	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java	.*: .*	cmd	ex.getLocalizedMessage()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java	.*: .*	cmd	e.getLocalizedMessage()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java	Failed to parse options	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java	Failed to start secondary namenode	e	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java	.*	pe.getMessage()	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java	Formatting storage directory .*	sd	
/Users/kjw/Work/logAnalysis/hadoop-trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/SecondaryNameNode.java	Failed to delete temporary edits file: .*	t.getAbsolutePath()	
